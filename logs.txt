* 
* ==> Audit <==
* |-----------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  |         User          | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 22:40 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 22:53 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 22:54 +01 |                     |
| stop      |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 22:55 +01 |                     |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:04 +01 | 06 Dec 23 23:05 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:05 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:14 +01 |                     |
| addons    | enable dashboard               | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:20 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:24 +01 |                     |
| stop      |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:29 +01 | 06 Dec 23 23:30 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:31 +01 | 06 Dec 23 23:33 +01 |
| ip        |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:33 +01 | 06 Dec 23 23:33 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 06 Dec 23 23:34 +01 |                     |
| stop      |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 11:41 +01 | 07 Dec 23 11:42 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 11:42 +01 | 07 Dec 23 11:44 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:18 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:22 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:24 +01 |                     |
| stop      |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:30 +01 | 07 Dec 23 12:30 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:31 +01 | 07 Dec 23 12:32 +01 |
| ip        |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:32 +01 | 07 Dec 23 12:32 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 12:33 +01 |                     |
| addons    | enable ingress                 | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 22:00 +01 | 07 Dec 23 22:01 +01 |
| addons    | enable ingress                 | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 07 Dec 23 22:15 +01 | 07 Dec 23 22:15 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 11:53 +01 | 08 Dec 23 11:54 +01 |
| addons    | enable ingress                 | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 11:55 +01 | 08 Dec 23 11:55 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 11:55 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 12:04 +01 |                     |
| ip        |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 12:09 +01 | 08 Dec 23 12:09 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 19:21 +01 | 08 Dec 23 19:22 +01 |
| stop      |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 22:58 +01 | 08 Dec 23 22:58 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 22:59 +01 | 08 Dec 23 23:00 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 22:59 +01 |                     |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 23:00 +01 | 08 Dec 23 23:15 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 23:03 +01 |                     |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 23:27 +01 | 08 Dec 23 23:32 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 08 Dec 23 23:56 +01 |                     |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:07 +01 |                     |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:09 +01 |                     |
| delete    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:10 +01 | 09 Dec 23 00:10 +01 |
| start     | --cpus 4 --memory 5000         | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:11 +01 |                     |
|           | --driver docker                |          |                       |         |                     |                     |
| start     | --cpus 4 --driver docker       | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:12 +01 | 09 Dec 23 00:13 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:16 +01 |                     |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:18 +01 | 09 Dec 23 00:28 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:19 +01 |                     |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:28 +01 | 09 Dec 23 00:30 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:30 +01 | 09 Dec 23 00:41 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:41 +01 | 09 Dec 23 00:43 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:43 +01 | 09 Dec 23 00:53 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:53 +01 | 09 Dec 23 00:54 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 00:54 +01 | 09 Dec 23 01:05 +01 |
| tunnel    |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 01:05 +01 | 09 Dec 23 01:19 +01 |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 01:11 +01 |                     |
| stop      |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 01:19 +01 | 09 Dec 23 01:19 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 01:19 +01 | 09 Dec 23 01:20 +01 |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 09:09 +01 |                     |
| start     |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 09:11 +01 | 09 Dec 23 09:12 +01 |
| addons    | enable                         | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 09:13 +01 |                     |
| addons    | enable ingress                 | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 09:13 +01 |                     |
| dashboard |                                | minikube | DESKTOP-1QAH816\abdel | v1.32.0 | 09 Dec 23 09:16 +01 |                     |
|-----------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/09 09:11:46
Running on machine: DESKTOP-1QAH816
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 09:11:46.780903   15476 out.go:296] Setting OutFile to fd 88 ...
I1209 09:11:46.782149   15476 out.go:348] isatty.IsTerminal(88) = true
I1209 09:11:46.782149   15476 out.go:309] Setting ErrFile to fd 92...
I1209 09:11:46.782149   15476 out.go:348] isatty.IsTerminal(92) = true
I1209 09:11:46.827235   15476 out.go:303] Setting JSON to false
I1209 09:11:46.835405   15476 start.go:128] hostinfo: {"hostname":"DESKTOP-1QAH816","uptime":77314,"bootTime":1702032192,"procs":298,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.3693 Build 19045.3693","kernelVersion":"10.0.19045.3693 Build 19045.3693","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"2384bccf-a8d2-4150-af7f-2bd7974883a7"}
W1209 09:11:46.835405   15476 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1209 09:11:46.842212   15476 out.go:177] 😄  minikube v1.32.0 on Microsoft Windows 10 Pro 10.0.19045.3693 Build 19045.3693
I1209 09:11:46.847219   15476 notify.go:220] Checking for updates...
I1209 09:11:46.847219   15476 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1209 09:11:46.851425   15476 driver.go:378] Setting default libvirt URI to qemu:///system
I1209 09:11:47.360642   15476 docker.go:122] docker version: linux-24.0.6:Docker Desktop 4.25.2 (129061)
I1209 09:11:47.379497   15476 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1209 09:11:50.793952   15476 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.4142519s)
I1209 09:11:50.797008   15476 info.go:266] docker info: {ID:3277c1c8-a86a-497d-bf2a-ffa15cc9a5b0 Containers:8 ContainersRunning:2 ContainersPaused:0 ContainersStopped:6 Images:27 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:true NGoroutines:99 SystemTime:2023-12-09 08:11:50.754738008 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4026511360 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:DESKTOP-1QAH816 Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I1209 09:11:50.798673   15476 out.go:177] ✨  Using the docker driver based on existing profile
I1209 09:11:50.800496   15476 start.go:298] selected driver: docker
I1209 09:11:50.800496   15476 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:4 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abdel:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1209 09:11:50.801020   15476 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1209 09:11:50.847676   15476 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1209 09:11:51.448798   15476 info.go:266] docker info: {ID:3277c1c8-a86a-497d-bf2a-ffa15cc9a5b0 Containers:8 ContainersRunning:2 ContainersPaused:0 ContainersStopped:6 Images:27 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:true NGoroutines:99 SystemTime:2023-12-09 08:11:51.402485321 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4026511360 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:DESKTOP-1QAH816 Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I1209 09:11:51.534153   15476 cni.go:84] Creating CNI manager for ""
I1209 09:11:51.534795   15476 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1209 09:11:51.534872   15476 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:4 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abdel:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1209 09:11:51.537507   15476 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1209 09:11:51.539492   15476 cache.go:121] Beginning downloading kic base image for docker with docker
I1209 09:11:51.540492   15476 out.go:177] 🚜  Pulling base image ...
I1209 09:11:51.542553   15476 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1209 09:11:51.542553   15476 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1209 09:11:51.542553   15476 preload.go:148] Found local preload: C:\Users\abdel\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1209 09:11:51.542553   15476 cache.go:56] Caching tarball of preloaded images
I1209 09:11:51.544493   15476 preload.go:174] Found C:\Users\abdel\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1209 09:11:51.544493   15476 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1209 09:11:51.546494   15476 profile.go:148] Saving config to C:\Users\abdel\.minikube\profiles\minikube\config.json ...
I1209 09:11:51.814024   15476 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I1209 09:11:51.814024   15476 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I1209 09:11:51.814562   15476 cache.go:194] Successfully downloaded all kic artifacts
I1209 09:11:51.815158   15476 start.go:365] acquiring machines lock for minikube: {Name:mk790b178658dd521a84995d1c9ff5fe85e13509 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1209 09:11:51.815682   15476 start.go:369] acquired machines lock for "minikube" in 523.5µs
I1209 09:11:51.815767   15476 start.go:96] Skipping create...Using existing machine configuration
I1209 09:11:51.816508   15476 fix.go:54] fixHost starting: 
I1209 09:11:51.850254   15476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 09:11:52.039141   15476 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1209 09:11:52.039322   15476 fix.go:128] unexpected machine state, will restart: <nil>
I1209 09:11:52.042189   15476 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1209 09:11:52.062705   15476 cli_runner.go:164] Run: docker start minikube
I1209 09:11:52.946232   15476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 09:11:53.159919   15476 kic.go:430] container "minikube" state is running.
I1209 09:11:53.195827   15476 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1209 09:11:53.406427   15476 profile.go:148] Saving config to C:\Users\abdel\.minikube\profiles\minikube\config.json ...
I1209 09:11:53.409036   15476 machine.go:88] provisioning docker machine ...
I1209 09:11:53.410046   15476 ubuntu.go:169] provisioning hostname "minikube"
I1209 09:11:53.428928   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:11:53.657623   15476 main.go:141] libmachine: Using SSH client type: native
I1209 09:11:53.661397   15476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7247e0] 0x727320 <nil>  [] 0s} 127.0.0.1 50944 <nil> <nil>}
I1209 09:11:53.661397   15476 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1209 09:11:53.668283   15476 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1209 09:11:56.977507   15476 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1209 09:11:56.998191   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:11:57.201572   15476 main.go:141] libmachine: Using SSH client type: native
I1209 09:11:57.202742   15476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7247e0] 0x727320 <nil>  [] 0s} 127.0.0.1 50944 <nil> <nil>}
I1209 09:11:57.202742   15476 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1209 09:11:57.434599   15476 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1209 09:11:57.436252   15476 ubuntu.go:175] set auth options {CertDir:C:\Users\abdel\.minikube CaCertPath:C:\Users\abdel\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\abdel\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\abdel\.minikube\machines\server.pem ServerKeyPath:C:\Users\abdel\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\abdel\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\abdel\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\abdel\.minikube}
I1209 09:11:57.436252   15476 ubuntu.go:177] setting up certificates
I1209 09:11:57.436252   15476 provision.go:83] configureAuth start
I1209 09:11:57.452947   15476 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1209 09:11:57.657362   15476 provision.go:138] copyHostCerts
I1209 09:11:57.661630   15476 exec_runner.go:144] found C:\Users\abdel\.minikube/ca.pem, removing ...
I1209 09:11:57.662298   15476 exec_runner.go:203] rm: C:\Users\abdel\.minikube\ca.pem
I1209 09:11:57.662565   15476 exec_runner.go:151] cp: C:\Users\abdel\.minikube\certs\ca.pem --> C:\Users\abdel\.minikube/ca.pem (1074 bytes)
I1209 09:11:57.665652   15476 exec_runner.go:144] found C:\Users\abdel\.minikube/cert.pem, removing ...
I1209 09:11:57.665672   15476 exec_runner.go:203] rm: C:\Users\abdel\.minikube\cert.pem
I1209 09:11:57.665692   15476 exec_runner.go:151] cp: C:\Users\abdel\.minikube\certs\cert.pem --> C:\Users\abdel\.minikube/cert.pem (1119 bytes)
I1209 09:11:57.667926   15476 exec_runner.go:144] found C:\Users\abdel\.minikube/key.pem, removing ...
I1209 09:11:57.667926   15476 exec_runner.go:203] rm: C:\Users\abdel\.minikube\key.pem
I1209 09:11:57.668453   15476 exec_runner.go:151] cp: C:\Users\abdel\.minikube\certs\key.pem --> C:\Users\abdel\.minikube/key.pem (1679 bytes)
I1209 09:11:57.669588   15476 provision.go:112] generating server cert: C:\Users\abdel\.minikube\machines\server.pem ca-key=C:\Users\abdel\.minikube\certs\ca.pem private-key=C:\Users\abdel\.minikube\certs\ca-key.pem org=abdel.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1209 09:11:58.202505   15476 provision.go:172] copyRemoteCerts
I1209 09:11:58.242372   15476 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1209 09:11:58.253968   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:11:58.461283   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:11:58.612543   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1209 09:11:58.697573   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1209 09:11:58.765215   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I1209 09:11:58.825312   15476 provision.go:86] duration metric: configureAuth took 1.3890598s
I1209 09:11:58.825312   15476 ubuntu.go:193] setting minikube options for container-runtime
I1209 09:11:58.827206   15476 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1209 09:11:58.845691   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:11:59.067288   15476 main.go:141] libmachine: Using SSH client type: native
I1209 09:11:59.067909   15476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7247e0] 0x727320 <nil>  [] 0s} 127.0.0.1 50944 <nil> <nil>}
I1209 09:11:59.067909   15476 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1209 09:11:59.253551   15476 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1209 09:11:59.253551   15476 ubuntu.go:71] root file system type: overlay
I1209 09:11:59.255079   15476 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1209 09:11:59.281310   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:11:59.488449   15476 main.go:141] libmachine: Using SSH client type: native
I1209 09:11:59.489105   15476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7247e0] 0x727320 <nil>  [] 0s} 127.0.0.1 50944 <nil> <nil>}
I1209 09:11:59.489105   15476 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1209 09:11:59.684413   15476 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1209 09:11:59.702810   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:11:59.956786   15476 main.go:141] libmachine: Using SSH client type: native
I1209 09:11:59.957746   15476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7247e0] 0x727320 <nil>  [] 0s} 127.0.0.1 50944 <nil> <nil>}
I1209 09:11:59.957746   15476 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1209 09:12:00.198438   15476 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1209 09:12:00.199433   15476 machine.go:91] provisioned docker machine in 6.789402s
I1209 09:12:00.204502   15476 start.go:300] post-start starting for "minikube" (driver="docker")
I1209 09:12:00.205488   15476 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1209 09:12:00.254934   15476 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1209 09:12:00.270653   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:00.467750   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:00.659505   15476 ssh_runner.go:195] Run: cat /etc/os-release
I1209 09:12:00.669086   15476 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1209 09:12:00.669086   15476 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1209 09:12:00.669086   15476 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1209 09:12:00.669086   15476 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1209 09:12:00.669864   15476 filesync.go:126] Scanning C:\Users\abdel\.minikube\addons for local assets ...
I1209 09:12:00.670414   15476 filesync.go:126] Scanning C:\Users\abdel\.minikube\files for local assets ...
I1209 09:12:00.670933   15476 start.go:303] post-start completed in 466.4314ms
I1209 09:12:00.702570   15476 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1209 09:12:00.721830   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:00.916172   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:01.140918   15476 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1209 09:12:01.156241   15476 fix.go:56] fixHost completed within 9.3399651s
I1209 09:12:01.156241   15476 start.go:83] releasing machines lock for "minikube", held for 9.3405365s
I1209 09:12:01.173178   15476 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1209 09:12:01.338186   15476 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1209 09:12:01.352786   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:01.370945   15476 ssh_runner.go:195] Run: cat /version.json
I1209 09:12:01.390176   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:01.529999   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:01.560818   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:02.844946   15476 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.5067598s)
I1209 09:12:02.844946   15476 ssh_runner.go:235] Completed: cat /version.json: (1.4740009s)
I1209 09:12:02.902373   15476 ssh_runner.go:195] Run: systemctl --version
I1209 09:12:02.953060   15476 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1209 09:12:02.998918   15476 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1209 09:12:03.024389   15476 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1209 09:12:03.082493   15476 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1209 09:12:03.107081   15476 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1209 09:12:03.107081   15476 start.go:472] detecting cgroup driver to use...
I1209 09:12:03.107081   15476 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1209 09:12:03.114457   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1209 09:12:03.189874   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1209 09:12:03.256772   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1209 09:12:03.282225   15476 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1209 09:12:03.314593   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1209 09:12:03.383996   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1209 09:12:03.473539   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1209 09:12:03.551812   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1209 09:12:03.614774   15476 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1209 09:12:03.684976   15476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1209 09:12:03.747981   15476 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1209 09:12:03.838830   15476 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1209 09:12:03.916803   15476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 09:12:04.164165   15476 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1209 09:12:04.380579   15476 start.go:472] detecting cgroup driver to use...
I1209 09:12:04.380579   15476 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1209 09:12:04.415474   15476 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1209 09:12:04.444660   15476 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1209 09:12:04.493558   15476 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1209 09:12:04.521694   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1209 09:12:04.606859   15476 ssh_runner.go:195] Run: which cri-dockerd
I1209 09:12:04.694601   15476 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1209 09:12:04.722525   15476 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1209 09:12:04.830852   15476 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1209 09:12:05.061259   15476 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1209 09:12:05.288795   15476 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1209 09:12:05.295081   15476 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1209 09:12:05.355151   15476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 09:12:05.573347   15476 ssh_runner.go:195] Run: sudo systemctl restart docker
I1209 09:12:06.332229   15476 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1209 09:12:06.588068   15476 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1209 09:12:06.854402   15476 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1209 09:12:07.084036   15476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 09:12:07.315078   15476 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1209 09:12:07.381839   15476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 09:12:07.623865   15476 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1209 09:12:08.209145   15476 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1209 09:12:08.267240   15476 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1209 09:12:08.279025   15476 start.go:540] Will wait 60s for crictl version
I1209 09:12:08.312216   15476 ssh_runner.go:195] Run: which crictl
I1209 09:12:08.353302   15476 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1209 09:12:08.669290   15476 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1209 09:12:08.691523   15476 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1209 09:12:08.893593   15476 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1209 09:12:08.961843   15476 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1209 09:12:08.976211   15476 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1209 09:12:09.413052   15476 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1209 09:12:09.471687   15476 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1209 09:12:09.483088   15476 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1209 09:12:09.541519   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1209 09:12:09.730270   15476 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1209 09:12:09.743840   15476 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1209 09:12:09.795851   15476 docker.go:671] Got preloaded images: -- stdout --
abdellahah/frontend-image:1.0.2
abdellahah/frontend-image:1.0.3
abdellahah/backend-image:1.0.3
abdellahah/backend-image:1.0.2
abdellahah/frontend-image:1.0.1
abdellahah/backend-image:1.0.1
mysql:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1209 09:12:09.797309   15476 docker.go:601] Images already preloaded, skipping extraction
I1209 09:12:09.814263   15476 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1209 09:12:09.864895   15476 docker.go:671] Got preloaded images: -- stdout --
abdellahah/frontend-image:1.0.2
abdellahah/frontend-image:1.0.3
abdellahah/backend-image:1.0.3
abdellahah/backend-image:1.0.2
abdellahah/frontend-image:1.0.1
abdellahah/backend-image:1.0.1
mysql:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1209 09:12:09.864895   15476 cache_images.go:84] Images are preloaded, skipping loading
I1209 09:12:09.876923   15476 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1209 09:12:10.363411   15476 cni.go:84] Creating CNI manager for ""
I1209 09:12:10.368373   15476 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1209 09:12:10.369576   15476 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1209 09:12:10.370095   15476 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1209 09:12:10.370674   15476 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1209 09:12:10.371235   15476 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1209 09:12:10.413820   15476 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1209 09:12:10.448393   15476 binaries.go:44] Found k8s binaries, skipping transfer
I1209 09:12:10.493697   15476 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1209 09:12:10.526241   15476 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1209 09:12:10.575068   15476 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1209 09:12:10.614371   15476 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1209 09:12:10.688816   15476 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1209 09:12:10.699616   15476 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1209 09:12:10.735949   15476 certs.go:56] Setting up C:\Users\abdel\.minikube\profiles\minikube for IP: 192.168.49.2
I1209 09:12:10.735997   15476 certs.go:190] acquiring lock for shared ca certs: {Name:mk7377a17e4e7a62bb6e1903e14afe8d1cc25f74 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 09:12:10.738048   15476 certs.go:199] skipping minikubeCA CA generation: C:\Users\abdel\.minikube\ca.key
I1209 09:12:10.740512   15476 certs.go:199] skipping proxyClientCA CA generation: C:\Users\abdel\.minikube\proxy-client-ca.key
I1209 09:12:10.746219   15476 certs.go:315] skipping minikube-user signed cert generation: C:\Users\abdel\.minikube\profiles\minikube\client.key
I1209 09:12:10.747433   15476 certs.go:315] skipping minikube signed cert generation: C:\Users\abdel\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1209 09:12:10.749030   15476 certs.go:315] skipping aggregator signed cert generation: C:\Users\abdel\.minikube\profiles\minikube\proxy-client.key
I1209 09:12:10.752045   15476 certs.go:437] found cert: C:\Users\abdel\.minikube\certs\C:\Users\abdel\.minikube\certs\ca-key.pem (1679 bytes)
I1209 09:12:10.752045   15476 certs.go:437] found cert: C:\Users\abdel\.minikube\certs\C:\Users\abdel\.minikube\certs\ca.pem (1074 bytes)
I1209 09:12:10.753045   15476 certs.go:437] found cert: C:\Users\abdel\.minikube\certs\C:\Users\abdel\.minikube\certs\cert.pem (1119 bytes)
I1209 09:12:10.753045   15476 certs.go:437] found cert: C:\Users\abdel\.minikube\certs\C:\Users\abdel\.minikube\certs\key.pem (1679 bytes)
I1209 09:12:10.761942   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1209 09:12:10.810657   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1209 09:12:10.886846   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1209 09:12:10.978041   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1209 09:12:11.033753   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1209 09:12:11.087757   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1209 09:12:11.147858   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1209 09:12:11.209374   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1209 09:12:11.285087   15476 ssh_runner.go:362] scp C:\Users\abdel\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1209 09:12:11.377224   15476 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1209 09:12:11.453462   15476 ssh_runner.go:195] Run: openssl version
I1209 09:12:11.555616   15476 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1209 09:12:11.625822   15476 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1209 09:12:11.643927   15476 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Dec  6 12:56 /usr/share/ca-certificates/minikubeCA.pem
I1209 09:12:11.682823   15476 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1209 09:12:11.746897   15476 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1209 09:12:11.811787   15476 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1209 09:12:11.865425   15476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1209 09:12:11.920629   15476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1209 09:12:11.978665   15476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1209 09:12:12.041958   15476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1209 09:12:12.113412   15476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1209 09:12:12.187686   15476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1209 09:12:12.218270   15476 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:4 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abdel:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1209 09:12:12.234253   15476 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1209 09:12:12.360083   15476 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1209 09:12:12.397334   15476 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1209 09:12:12.403999   15476 kubeadm.go:636] restartCluster start
I1209 09:12:12.432884   15476 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1209 09:12:12.456742   15476 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1209 09:12:12.477730   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1209 09:12:12.720001   15476 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:58803"
I1209 09:12:12.720593   15476 kubeconfig.go:135] verify returned: got: 127.0.0.1:58803, want: 127.0.0.1:50948
I1209 09:12:12.723814   15476 lock.go:35] WriteFile acquiring C:\Users\abdel\.kube\config: {Name:mk5d02a7e70e42dd3997ee64fb5475325d2f1a0f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 09:12:12.828914   15476 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1209 09:12:12.851917   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:12.885077   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:12.910905   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:12.910905   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:12.953309   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:12.991088   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:13.492422   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:13.541751   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:13.569546   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:13.994122   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:14.038789   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:14.066160   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:14.492100   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:14.547841   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:14.578931   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:14.992275   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:15.053881   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:15.075583   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:15.493836   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:15.555926   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:15.583169   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:16.017951   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:16.088212   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:16.122832   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:16.493482   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:16.555699   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:16.620776   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:16.991711   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:17.046884   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:17.080205   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:17.492275   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:17.569440   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:17.595833   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:17.991687   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:18.057236   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:18.080795   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:18.491831   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:18.548177   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:18.593877   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:18.992744   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:19.031610   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:19.057445   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:19.492779   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:19.542685   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:19.575132   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:19.992621   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:20.117510   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:20.143813   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:20.492297   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:20.585310   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:20.624687   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:20.992222   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:21.127671   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:21.165321   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:21.492921   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:21.525839   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:21.568266   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:21.991640   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:22.057240   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:22.121750   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:22.492187   15476 api_server.go:166] Checking apiserver status ...
I1209 09:12:22.578805   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 09:12:22.626287   15476 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 09:12:22.852441   15476 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1209 09:12:22.852972   15476 kubeadm.go:1128] stopping kube-system containers ...
I1209 09:12:22.869715   15476 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1209 09:12:22.942354   15476 docker.go:469] Stopping containers: [b37ad791c5d7 7fcc14cb809d b1dd8e20c361 8e309df65b6f 797fb10c7ed1 559e7556b6e3 2b9dfa13aa4c 259f877b7f95 042f2ee1d940 adaed48db95b 9986e447445a 686ef6720545 0dd7bbeabac8 6a38739700e8 1471a6b295b0 6a42df27b064 7358350ea982 92da36a54894 5a21c87c3248 e58b55aacc27 cd612be2ad02 4bd4702fcc16 d9f6d415cc86 f0efff296612 3a4491b50a27 2e28d0ebdc6a e10b8ec41db1]
I1209 09:12:22.962099   15476 ssh_runner.go:195] Run: docker stop b37ad791c5d7 7fcc14cb809d b1dd8e20c361 8e309df65b6f 797fb10c7ed1 559e7556b6e3 2b9dfa13aa4c 259f877b7f95 042f2ee1d940 adaed48db95b 9986e447445a 686ef6720545 0dd7bbeabac8 6a38739700e8 1471a6b295b0 6a42df27b064 7358350ea982 92da36a54894 5a21c87c3248 e58b55aacc27 cd612be2ad02 4bd4702fcc16 d9f6d415cc86 f0efff296612 3a4491b50a27 2e28d0ebdc6a e10b8ec41db1
I1209 09:12:23.084320   15476 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1209 09:12:23.150836   15476 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1209 09:12:23.173852   15476 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Dec  8 23:13 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Dec  9 00:20 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Dec  8 23:13 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Dec  9 00:20 /etc/kubernetes/scheduler.conf

I1209 09:12:23.219369   15476 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1209 09:12:23.286168   15476 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1209 09:12:23.351309   15476 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1209 09:12:23.379397   15476 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1209 09:12:23.421676   15476 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1209 09:12:23.486408   15476 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1209 09:12:23.521269   15476 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1209 09:12:23.559119   15476 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1209 09:12:23.653973   15476 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1209 09:12:23.684477   15476 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1209 09:12:23.684695   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1209 09:12:24.252963   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1209 09:12:26.721370   15476 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.4683672s)
I1209 09:12:26.721370   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1209 09:12:27.136096   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1209 09:12:27.272798   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1209 09:12:27.414927   15476 api_server.go:52] waiting for apiserver process to appear ...
I1209 09:12:27.465120   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:27.566895   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:28.206096   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:28.756441   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:29.210602   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:29.744677   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:30.176807   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:30.688746   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:31.293658   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:31.671910   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:32.173638   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:32.652045   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:33.193387   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:33.242091   15476 api_server.go:72] duration metric: took 5.8271646s to wait for apiserver process to appear ...
I1209 09:12:33.242091   15476 api_server.go:88] waiting for apiserver healthz status ...
I1209 09:12:33.243105   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:33.255107   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:33.255107   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:33.259682   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:33.760195   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:33.778027   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:34.265590   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:34.277746   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:34.760663   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:34.767628   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:35.261276   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:35.270242   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:35.761109   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:35.766573   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:36.260235   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:36.266299   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": EOF
I1209 09:12:36.782027   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:41.798439   15476 api_server.go:269] stopped: https://127.0.0.1:50948/healthz: Get "https://127.0.0.1:50948/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1209 09:12:41.799080   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:42.306068   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1209 09:12:42.307334   15476 api_server.go:103] status: https://127.0.0.1:50948/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1209 09:12:42.307334   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:42.491319   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1209 09:12:42.491319   15476 api_server.go:103] status: https://127.0.0.1:50948/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1209 09:12:42.759892   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:42.778039   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1209 09:12:42.778039   15476 api_server.go:103] status: https://127.0.0.1:50948/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1209 09:12:43.260295   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:43.287288   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1209 09:12:43.287288   15476 api_server.go:103] status: https://127.0.0.1:50948/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1209 09:12:43.762854   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:43.783301   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1209 09:12:43.783301   15476 api_server.go:103] status: https://127.0.0.1:50948/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1209 09:12:44.261076   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:44.284378   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 200:
ok
I1209 09:12:44.332245   15476 api_server.go:141] control plane version: v1.28.3
I1209 09:12:44.332245   15476 api_server.go:131] duration metric: took 11.0901533s to wait for apiserver health ...
I1209 09:12:44.332245   15476 cni.go:84] Creating CNI manager for ""
I1209 09:12:44.332245   15476 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1209 09:12:44.334157   15476 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1209 09:12:44.434357   15476 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1209 09:12:44.472845   15476 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1209 09:12:44.537959   15476 system_pods.go:43] waiting for kube-system pods to appear ...
I1209 09:12:44.588786   15476 system_pods.go:59] 7 kube-system pods found
I1209 09:12:44.588786   15476 system_pods.go:61] "coredns-5dd5756b68-bbmkk" [a22fa65b-2e40-4c82-9b6b-6b67690fd8ab] Running
I1209 09:12:44.588786   15476 system_pods.go:61] "etcd-minikube" [507a3e18-0f9b-4384-b9b6-89e1736106ff] Running
I1209 09:12:44.588786   15476 system_pods.go:61] "kube-apiserver-minikube" [0bed6f82-b14c-4535-ae5b-e40c03d04d8b] Running
I1209 09:12:44.588786   15476 system_pods.go:61] "kube-controller-manager-minikube" [b86f2d4f-3c28-4d5f-924f-c5a13956fbe0] Running
I1209 09:12:44.588786   15476 system_pods.go:61] "kube-proxy-spn9l" [f232c395-a6a1-4eb5-a61e-50844fc648c1] Running
I1209 09:12:44.588786   15476 system_pods.go:61] "kube-scheduler-minikube" [43c55b4d-fa0a-492e-b7ba-13dc4439d464] Running
I1209 09:12:44.588786   15476 system_pods.go:61] "storage-provisioner" [7e0d9abb-ada7-4b20-85f1-b5b36e4bc13b] Running
I1209 09:12:44.588786   15476 system_pods.go:74] duration metric: took 50.827ms to wait for pod list to return data ...
I1209 09:12:44.588786   15476 node_conditions.go:102] verifying NodePressure condition ...
I1209 09:12:44.597485   15476 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1209 09:12:44.597485   15476 node_conditions.go:123] node cpu capacity is 8
I1209 09:12:44.598026   15476 node_conditions.go:105] duration metric: took 9.24ms to run NodePressure ...
I1209 09:12:44.598711   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1209 09:12:44.977128   15476 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1209 09:12:44.996418   15476 ops.go:34] apiserver oom_adj: -16
I1209 09:12:44.997507   15476 kubeadm.go:640] restartCluster took 32.5924184s
I1209 09:12:44.997533   15476 kubeadm.go:406] StartCluster complete in 32.7792681s
I1209 09:12:44.997575   15476 settings.go:142] acquiring lock: {Name:mk2fbe948f04c233c16564623c6be5feb6ce7645 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 09:12:44.998112   15476 settings.go:150] Updating kubeconfig:  C:\Users\abdel\.kube\config
I1209 09:12:45.000538   15476 lock.go:35] WriteFile acquiring C:\Users\abdel\.kube\config: {Name:mk5d02a7e70e42dd3997ee64fb5475325d2f1a0f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 09:12:45.003425   15476 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1209 09:12:45.003425   15476 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1209 09:12:45.004060   15476 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1209 09:12:45.005134   15476 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1209 09:12:45.005152   15476 addons.go:69] Setting dashboard=true in profile "minikube"
I1209 09:12:45.005175   15476 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1209 09:12:45.005175   15476 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1209 09:12:45.005175   15476 addons.go:231] Setting addon dashboard=true in "minikube"
W1209 09:12:45.005175   15476 addons.go:240] addon dashboard should already be in state true
I1209 09:12:45.005175   15476 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1209 09:12:45.005175   15476 addons.go:240] addon storage-provisioner should already be in state true
I1209 09:12:45.006053   15476 host.go:66] Checking if "minikube" exists ...
I1209 09:12:45.006053   15476 host.go:66] Checking if "minikube" exists ...
I1209 09:12:45.050426   15476 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1209 09:12:45.051556   15476 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1209 09:12:45.052959   15476 out.go:177] 🔎  Verifying Kubernetes components...
I1209 09:12:45.058798   15476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 09:12:45.059354   15476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 09:12:45.060769   15476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 09:12:45.172157   15476 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1209 09:12:45.642231   15476 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1209 09:12:45.644463   15476 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1209 09:12:45.646553   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1209 09:12:45.646553   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1209 09:12:45.693539   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:45.709022   15476 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1209 09:12:45.707750   15476 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1209 09:12:45.711165   15476 addons.go:240] addon default-storageclass should already be in state true
I1209 09:12:45.712214   15476 host.go:66] Checking if "minikube" exists ...
I1209 09:12:45.716625   15476 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1209 09:12:45.716625   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1209 09:12:45.758222   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:45.820566   15476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 09:12:46.134685   15476 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1209 09:12:46.134685   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1209 09:12:46.151311   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:46.159921   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:46.161325   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 09:12:46.723601   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1209 09:12:46.723601   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1209 09:12:46.788094   15476 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1209 09:12:46.813062   15476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50944 SSHKeyPath:C:\Users\abdel\.minikube\machines\minikube\id_rsa Username:docker}
I1209 09:12:46.894236   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1209 09:12:46.894236   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1209 09:12:46.997742   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1209 09:12:46.997742   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1209 09:12:47.111849   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1209 09:12:47.111849   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1209 09:12:47.176674   15476 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2.1732486s)
I1209 09:12:47.177673   15476 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1209 09:12:47.177673   15476 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2.0055163s)
I1209 09:12:47.225711   15476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1209 09:12:47.226715   15476 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1209 09:12:47.288110   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I1209 09:12:47.288110   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1209 09:12:47.469595   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1209 09:12:47.469595   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1209 09:12:47.567718   15476 api_server.go:52] waiting for apiserver process to appear ...
I1209 09:12:47.624937   15476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 09:12:47.717183   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1209 09:12:47.717183   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1209 09:12:47.886222   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1209 09:12:47.886222   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1209 09:12:48.082444   15476 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1209 09:12:48.082444   15476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1209 09:12:48.246023   15476 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1209 09:12:54.115165   15476 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.3270712s)
I1209 09:12:54.115165   15476 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.8884499s)
I1209 09:12:54.115165   15476 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (6.4902276s)
I1209 09:12:54.115165   15476 api_server.go:72] duration metric: took 9.0636087s to wait for apiserver process to appear ...
I1209 09:12:54.115165   15476 api_server.go:88] waiting for apiserver healthz status ...
I1209 09:12:54.115165   15476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50948/healthz ...
I1209 09:12:54.214550   15476 api_server.go:279] https://127.0.0.1:50948/healthz returned 200:
ok
I1209 09:12:54.232146   15476 api_server.go:141] control plane version: v1.28.3
I1209 09:12:54.232146   15476 api_server.go:131] duration metric: took 116.9811ms to wait for apiserver health ...
I1209 09:12:54.232146   15476 system_pods.go:43] waiting for kube-system pods to appear ...
I1209 09:12:54.340132   15476 system_pods.go:59] 7 kube-system pods found
I1209 09:12:54.340132   15476 system_pods.go:61] "coredns-5dd5756b68-bbmkk" [a22fa65b-2e40-4c82-9b6b-6b67690fd8ab] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1209 09:12:54.340132   15476 system_pods.go:61] "etcd-minikube" [507a3e18-0f9b-4384-b9b6-89e1736106ff] Running
I1209 09:12:54.340132   15476 system_pods.go:61] "kube-apiserver-minikube" [0bed6f82-b14c-4535-ae5b-e40c03d04d8b] Running
I1209 09:12:54.340132   15476 system_pods.go:61] "kube-controller-manager-minikube" [b86f2d4f-3c28-4d5f-924f-c5a13956fbe0] Running
I1209 09:12:54.340132   15476 system_pods.go:61] "kube-proxy-spn9l" [f232c395-a6a1-4eb5-a61e-50844fc648c1] Running
I1209 09:12:54.340132   15476 system_pods.go:61] "kube-scheduler-minikube" [43c55b4d-fa0a-492e-b7ba-13dc4439d464] Running
I1209 09:12:54.340132   15476 system_pods.go:61] "storage-provisioner" [7e0d9abb-ada7-4b20-85f1-b5b36e4bc13b] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1209 09:12:54.340132   15476 system_pods.go:74] duration metric: took 107.9859ms to wait for pod list to return data ...
I1209 09:12:54.340132   15476 kubeadm.go:581] duration metric: took 9.2885757s to wait for : map[apiserver:true system_pods:true] ...
I1209 09:12:54.340132   15476 node_conditions.go:102] verifying NodePressure condition ...
I1209 09:12:54.354506   15476 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1209 09:12:54.354506   15476 node_conditions.go:123] node cpu capacity is 8
I1209 09:12:54.354506   15476 node_conditions.go:105] duration metric: took 14.3738ms to run NodePressure ...
I1209 09:12:54.354506   15476 start.go:228] waiting for startup goroutines ...
I1209 09:12:55.950189   15476 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (7.7031687s)
I1209 09:12:55.951467   15476 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1209 09:12:55.956193   15476 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard
I1209 09:12:55.957467   15476 addons.go:502] enable addons completed in 10.9545968s: enabled=[storage-provisioner default-storageclass dashboard]
I1209 09:12:55.957467   15476 start.go:233] waiting for cluster config update ...
I1209 09:12:55.957467   15476 start.go:242] writing updated cluster config ...
I1209 09:12:56.038250   15476 ssh_runner.go:195] Run: rm -f paused
I1209 09:12:56.572231   15476 start.go:600] kubectl: 1.28.2, cluster: 1.28.3 (minor skew: 0)
I1209 09:12:56.573548   15476 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Dec 09 08:14:23 minikube cri-dockerd[1274]: time="2023-12-09T08:14:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=========>                                         ]  4.297MB/22.19MB"
Dec 09 08:14:33 minikube cri-dockerd[1274]: time="2023-12-09T08:14:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============>                                     ]  5.881MB/22.19MB"
Dec 09 08:14:43 minikube cri-dockerd[1274]: time="2023-12-09T08:14:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [==================>                                ]  8.144MB/22.19MB"
Dec 09 08:14:53 minikube cri-dockerd[1274]: time="2023-12-09T08:14:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=====================>                             ]  9.502MB/22.19MB"
Dec 09 08:15:03 minikube cri-dockerd[1274]: time="2023-12-09T08:15:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=======================>                           ]  10.41MB/22.19MB"
Dec 09 08:15:13 minikube cri-dockerd[1274]: time="2023-12-09T08:15:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=======================>                           ]  10.63MB/22.19MB"
Dec 09 08:15:23 minikube cri-dockerd[1274]: time="2023-12-09T08:15:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [==========================>                        ]  11.77MB/22.19MB"
Dec 09 08:15:33 minikube cri-dockerd[1274]: time="2023-12-09T08:15:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [===========================>                       ]  12.22MB/22.19MB"
Dec 09 08:15:43 minikube cri-dockerd[1274]: time="2023-12-09T08:15:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:15:48 minikube cri-dockerd[1274]: time="2023-12-09T08:15:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/82105696643afe27b01c2a102e60754b3551ee4afeeaf7caf88fe3cbc8cdebfd/resolv.conf as [nameserver 10.96.0.10 search exam.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 08:15:50 minikube dockerd[1007]: time="2023-12-09T08:15:50.112578385Z" level=info msg="ignoring event" container=6ca214f6431a09b1a28029abba34848915b0b7a2c23a2cd8924704ed2f4b1b5e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:15:50 minikube dockerd[1007]: time="2023-12-09T08:15:50.470284085Z" level=info msg="ignoring event" container=1052aab8b1ee8c1d94f275926a11e51c2fe1084c3029555ea241fd6523f62c11 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:15:50 minikube cri-dockerd[1274]: time="2023-12-09T08:15:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b7ab4edc6dba6284351483016755cf40fd1b99da3f57d645d5f830938f56cc3f/resolv.conf as [nameserver 10.96.0.10 search exam.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 08:15:52 minikube dockerd[1007]: time="2023-12-09T08:15:52.075308887Z" level=info msg="ignoring event" container=b34c86183439431963c83d4d4092bbb146035ecb151bed3fa67aa409648386b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:15:52 minikube dockerd[1007]: time="2023-12-09T08:15:52.395806973Z" level=info msg="ignoring event" container=25ee579e831e287d6cd18616aa2825ec66e77fa15846820a97efe6993a38c815 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:15:53 minikube cri-dockerd[1274]: time="2023-12-09T08:15:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:15:57 minikube cri-dockerd[1274]: time="2023-12-09T08:15:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e47bc9ee07f1c81ea85bad5136c7f0148fa0125025e6ef776cf0a31fc8633ae/resolv.conf as [nameserver 10.96.0.10 search exam.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 08:15:58 minikube dockerd[1007]: time="2023-12-09T08:15:58.469910356Z" level=info msg="ignoring event" container=c12d08bf12b59d475fed933f16e3d9f776c17bab8fcb4d2dc27711bf48c0c34b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:15:58 minikube dockerd[1007]: time="2023-12-09T08:15:58.986108660Z" level=info msg="ignoring event" container=c088a7d13d2b9f76e3699b51f0c3899f53278e357675c4a36620d11034d999f2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:15:59 minikube cri-dockerd[1274]: time="2023-12-09T08:15:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6d839ebe1c489cb53737d2763e1042a8db51e1eae0aef7648b71abd65ac3c935/resolv.conf as [nameserver 10.96.0.10 search exam.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 08:16:01 minikube dockerd[1007]: time="2023-12-09T08:16:01.227336673Z" level=info msg="ignoring event" container=46c9ad558f9c2086823f0e95c97c503eb22d43a6af9ea75ae965c989dfdb0632 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:16:01 minikube dockerd[1007]: time="2023-12-09T08:16:01.604817418Z" level=info msg="ignoring event" container=ba3c0306e8c98ab8bf265a50e35658b1cc435a85a6ab89911ae80dfba739c46c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 08:16:03 minikube cri-dockerd[1274]: time="2023-12-09T08:16:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:16:13 minikube cri-dockerd[1274]: time="2023-12-09T08:16:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:16:23 minikube cri-dockerd[1274]: time="2023-12-09T08:16:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:16:33 minikube cri-dockerd[1274]: time="2023-12-09T08:16:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:16:43 minikube cri-dockerd[1274]: time="2023-12-09T08:16:43Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80 because it exceeded image pull deadline 1m0s. Latest progress 2c4dd5b46232: Downloading [=============================>                     ]   12.9MB/22.19MB"
Dec 09 08:16:43 minikube dockerd[1007]: time="2023-12-09T08:16:43.648902021Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 09 08:16:53 minikube dockerd[1007]: time="2023-12-09T08:16:53.778289319Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:16:53 minikube dockerd[1007]: time="2023-12-09T08:16:53.778709072Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:16:53 minikube dockerd[1007]: time="2023-12-09T08:16:53.787126631Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:09 minikube dockerd[1007]: time="2023-12-09T08:17:09.607276918Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Dec 09 08:17:09 minikube dockerd[1007]: time="2023-12-09T08:17:09.607413016Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Dec 09 08:17:09 minikube dockerd[1007]: time="2023-12-09T08:17:09.614524533Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Dec 09 08:17:19 minikube dockerd[1007]: time="2023-12-09T08:17:19.663749686Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:19 minikube dockerd[1007]: time="2023-12-09T08:17:19.664010586Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:19 minikube dockerd[1007]: time="2023-12-09T08:17:19.678147018Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:44 minikube dockerd[1007]: time="2023-12-09T08:17:44.446455881Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:44 minikube dockerd[1007]: time="2023-12-09T08:17:44.446530281Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:44 minikube dockerd[1007]: time="2023-12-09T08:17:44.452546577Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:55 minikube dockerd[1007]: time="2023-12-09T08:17:55.450773835Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:55 minikube dockerd[1007]: time="2023-12-09T08:17:55.450893939Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:17:55 minikube dockerd[1007]: time="2023-12-09T08:17:55.456877147Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:18:49 minikube dockerd[1007]: time="2023-12-09T08:18:49.458563385Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Dec 09 08:18:49 minikube dockerd[1007]: time="2023-12-09T08:18:49.458694787Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Dec 09 08:18:49 minikube dockerd[1007]: time="2023-12-09T08:18:49.465604344Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Dec 09 08:18:59 minikube dockerd[1007]: time="2023-12-09T08:18:59.513965746Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:18:59 minikube dockerd[1007]: time="2023-12-09T08:18:59.514214021Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:18:59 minikube dockerd[1007]: time="2023-12-09T08:18:59.519279618Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:20:33 minikube cri-dockerd[1274]: time="2023-12-09T08:20:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Pulling fs layer "
Dec 09 08:20:43 minikube cri-dockerd[1274]: time="2023-12-09T08:20:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Pulling fs layer "
Dec 09 08:20:53 minikube cri-dockerd[1274]: time="2023-12-09T08:20:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Pulling fs layer "
Dec 09 08:21:03 minikube cri-dockerd[1274]: time="2023-12-09T08:21:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Pulling fs layer "
Dec 09 08:21:13 minikube cri-dockerd[1274]: time="2023-12-09T08:21:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Pulling fs layer "
Dec 09 08:21:23 minikube cri-dockerd[1274]: time="2023-12-09T08:21:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80: 2c4dd5b46232: Pulling fs layer "
Dec 09 08:21:33 minikube cri-dockerd[1274]: time="2023-12-09T08:21:33Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80 because it exceeded image pull deadline 1m0s. Latest progress 2c4dd5b46232: Pulling fs layer "
Dec 09 08:21:33 minikube dockerd[1007]: time="2023-12-09T08:21:33.075853450Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 09 08:21:43 minikube dockerd[1007]: time="2023-12-09T08:21:43.106964792Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:21:43 minikube dockerd[1007]: time="2023-12-09T08:21:43.107070093Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"
Dec 09 08:21:43 minikube dockerd[1007]: time="2023-12-09T08:21:43.111569438Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
73892e02c9e13       59ac2ad3e1c4a                                                                   7 minutes ago       Running             backend                     0                   6d839ebe1c489       backend-deployment-767bfdd69-tr5sg
6155234e5ce0d       59ac2ad3e1c4a                                                                   8 minutes ago       Running             backend                     0                   8e47bc9ee07f1       backend-deployment-767bfdd69-wdz72
88385d2591595       9cfbac4e0857f                                                                   8 minutes ago       Running             frontend                    0                   b7ab4edc6dba6       frontend-deployment-78697ddb7c-4d2cg
c49c35c5f6dee       9cfbac4e0857f                                                                   8 minutes ago       Running             frontend                    0                   82105696643af       frontend-deployment-78697ddb7c-6nsm2
51cf56730f4ec       07655ddf2eebe                                                                   10 minutes ago      Running             kubernetes-dashboard        4                   7a9187e25024c       kubernetes-dashboard-8694d4445c-zlmnc
95f47218fb3b4       6e38f40d628db                                                                   10 minutes ago      Running             storage-provisioner         5                   2210df37e3e91       storage-provisioner
10108ed1c8f8b       mysql@sha256:6057dec95d87a0d7880d9cfc9b3d9292f9c11473a5104b906402a2b73396e377   10 minutes ago      Running             mysql                       2                   09d9695c1d86c       mysql-statefulset-0
fe2ae5b8f851d       mysql@sha256:6057dec95d87a0d7880d9cfc9b3d9292f9c11473a5104b906402a2b73396e377   10 minutes ago      Running             mysql                       2                   6bcf866032261       mysql-statefulset-1
3fdd6c88d7010       115053965e86b                                                                   11 minutes ago      Running             dashboard-metrics-scraper   2                   3f9fab60986a0       dashboard-metrics-scraper-7fd5cb4ddc-p5nz6
99fc263356078       07655ddf2eebe                                                                   11 minutes ago      Exited              kubernetes-dashboard        3                   7a9187e25024c       kubernetes-dashboard-8694d4445c-zlmnc
c25d7ea7feb65       ead0a4a53df89                                                                   11 minutes ago      Running             coredns                     2                   215bdea3856bc       coredns-5dd5756b68-bbmkk
e259ef5e466aa       6e38f40d628db                                                                   11 minutes ago      Exited              storage-provisioner         4                   2210df37e3e91       storage-provisioner
e94d4c14d9b9f       bfc896cf80fba                                                                   11 minutes ago      Running             kube-proxy                  2                   f1f95af4850a2       kube-proxy-spn9l
b759b7c3a44ba       5374347291230                                                                   11 minutes ago      Running             kube-apiserver              2                   9b6fe1559e849       kube-apiserver-minikube
90db0a937b910       6d1b4fd1b182d                                                                   11 minutes ago      Running             kube-scheduler              2                   f0de168d1c0c4       kube-scheduler-minikube
32b392ce16e72       73deb9a3f7025                                                                   11 minutes ago      Running             etcd                        2                   ab201c9dbf1ff       etcd-minikube
d13654ff003b9       10baa1ca17068                                                                   11 minutes ago      Running             kube-controller-manager     2                   5746171485bc8       kube-controller-manager-minikube
19857b7e18ff1       mysql@sha256:6057dec95d87a0d7880d9cfc9b3d9292f9c11473a5104b906402a2b73396e377   8 hours ago         Exited              mysql                       1                   7ceb24468b4d5       mysql-statefulset-0
49a35baddec41       mysql@sha256:6057dec95d87a0d7880d9cfc9b3d9292f9c11473a5104b906402a2b73396e377   8 hours ago         Exited              mysql                       1                   00351183b511f       mysql-statefulset-1
340b09675f83c       115053965e86b                                                                   8 hours ago         Exited              dashboard-metrics-scraper   1                   fe54a97437331       dashboard-metrics-scraper-7fd5cb4ddc-p5nz6
7fcc14cb809d3       ead0a4a53df89                                                                   8 hours ago         Exited              coredns                     1                   797fb10c7ed19       coredns-5dd5756b68-bbmkk
b1dd8e20c361a       bfc896cf80fba                                                                   8 hours ago         Exited              kube-proxy                  1                   559e7556b6e3b       kube-proxy-spn9l
259f877b7f95d       6d1b4fd1b182d                                                                   8 hours ago         Exited              kube-scheduler              1                   0dd7bbeabac80       kube-scheduler-minikube
042f2ee1d940c       73deb9a3f7025                                                                   8 hours ago         Exited              etcd                        1                   686ef6720545e       etcd-minikube
adaed48db95b6       5374347291230                                                                   8 hours ago         Exited              kube-apiserver              1                   6a38739700e80       kube-apiserver-minikube
9986e447445ae       10baa1ca17068                                                                   8 hours ago         Exited              kube-controller-manager     1                   1471a6b295b01       kube-controller-manager-minikube

* 
* ==> coredns [7fcc14cb809d] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:55078 - 1861 "HINFO IN 7003385201477539607.4390465702195686420. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.109341929s
[INFO] 10.244.0.24:57359 - 7438 "A IN mysql-service.exam.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.001283906s
[INFO] 10.244.0.29:42261 - 54532 "A IN mysql-service.exam.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000271702s

* 
* ==> coredns [c25d7ea7feb6] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:55061 - 21506 "HINFO IN 4816715531459465328.1873021484906259934. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.313699125s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 10.244.0.37:37524 - 4509 "A IN mysql-service.exam.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.020961715s
[INFO] 10.244.0.34:37799 - 31049 "A IN mysql-service.exam.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.020445725s
[INFO] 10.244.0.45:44865 - 3892 "A IN mysql-service.exam.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.034953157s
[INFO] 10.244.0.46:45115 - 63987 "A IN mysql-service.exam.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.034559259s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_12_09T00_13_21_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 08 Dec 2023 23:13:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 09 Dec 2023 08:23:56 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 09 Dec 2023 08:22:57 +0000   Fri, 08 Dec 2023 23:13:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 09 Dec 2023 08:22:57 +0000   Fri, 08 Dec 2023 23:13:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 09 Dec 2023 08:22:57 +0000   Fri, 08 Dec 2023 23:13:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 09 Dec 2023 08:22:57 +0000   Fri, 08 Dec 2023 23:13:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3932140Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3932140Ki
  pods:               110
System Info:
  Machine ID:                 975907580aec4fa6bb17fee89edd701f
  System UUID:                975907580aec4fa6bb17fee89edd701f
  Boot ID:                    729f885c-d172-4988-a490-58104f1edd9d
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  exam                        backend-deployment-767bfdd69-tr5sg            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m1s
  exam                        backend-deployment-767bfdd69-wdz72            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m3s
  exam                        frontend-deployment-78697ddb7c-4d2cg          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m10s
  exam                        frontend-deployment-78697ddb7c-6nsm2          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m13s
  exam                        mysql-statefulset-0                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  exam                        mysql-statefulset-1                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  ingress-nginx               ingress-nginx-admission-create-xd5vp          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  ingress-nginx               ingress-nginx-admission-patch-m657d           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-dpkg2     100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         10m
  kube-system                 coredns-5dd5756b68-bbmkk                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     9h
  kube-system                 etcd-minikube                                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         9h
  kube-system                 kube-apiserver-minikube                       250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 kube-controller-manager-minikube              200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 kube-proxy-spn9l                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 kube-scheduler-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-p5nz6    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-zlmnc         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (6%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 11m                kube-proxy       
  Normal  Starting                 8h                 kube-proxy       
  Normal  NodeAllocatableEnforced  8h                 kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 8h                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  8h (x8 over 8h)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasSufficientPID     8h (x7 over 8h)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    8h (x8 over 8h)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  RegisteredNode           8h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 11m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  11m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           11m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Dec 9 08:10] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.002925] PCI: Fatal: No config space access function found
[  +0.032481] PCI: System does not support PCI
[  +0.038388] kvm: no hardware support
[  +0.000007] kvm: no hardware support
[  +4.172201] FS-Cache: Duplicate cookie detected
[  +0.002489] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.001845] FS-Cache: O-cookie d=000000002bb8ad15{9P.session} n=0000000087580f49
[  +0.002379] FS-Cache: O-key=[10] '34323934393337373230'
[  +0.001057] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.001654] FS-Cache: N-cookie d=000000002bb8ad15{9P.session} n=0000000086c8f5d4
[  +0.001027] FS-Cache: N-key=[10] '34323934393337373230'
[  +0.661045] 9pnet_virtio: no channels available for device drvfs
[  +0.001380] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.289790] WSL (1) ERROR: ConfigApplyWindowsLibPath:2529: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.043029] 9pnet_virtio: no channels available for device drvfs
[  +0.001100] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.101657] 9pnet_virtio: no channels available for device drvfs
[  +0.101313] 9pnet_virtio: no channels available for device drvfs
[  +0.140596] WSL (1) WARNING: /usr/share/zoneinfo/Africa/Casablanca not found. Is the tzdata package installed?
[  +0.449235] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001716] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001135] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001348] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.442885] 9pnet_virtio: no channels available for device drvfs
[  +0.001085] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.101058] 9pnet_virtio: no channels available for device drvfs
[  +0.101591] 9pnet_virtio: no channels available for device drvfs
[  +0.363291] WSL (2) ERROR: UtilCreateProcessAndWait:663: /bin/mount failed with 2
[  +0.002563] WSL (1) ERROR: UtilCreateProcessAndWait:685: /bin/mount failed with status 0xff00

[  +0.003633] WSL (1) ERROR: ConfigMountFsTab:2581: Processing fstab with mount -a failed.
[  +0.023971] WSL (1) ERROR: ConfigApplyWindowsLibPath:2529: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.055066] 9pnet_virtio: no channels available for device drvfs
[  +0.007378] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.129745] 9pnet_virtio: no channels available for device drvfs
[  +0.106629] 9pnet_virtio: no channels available for device drvfs
[  +0.457653] WSL (1) WARNING: /usr/share/zoneinfo/Africa/Casablanca not found. Is the tzdata package installed?
[  +0.043364] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.009103] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.025296] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.016604] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +4.249718] netlink: 'init': attribute type 4 has an invalid length.
[  +4.047366] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.

* 
* ==> etcd [042f2ee1d940] <==
* {"level":"info","ts":"2023-12-09T00:20:37.837946Z","caller":"traceutil/trace.go:171","msg":"trace[1465132326] transaction","detail":"{read_only:false; response_revision:4422; number_of_response:1; }","duration":"1.026773235s","start":"2023-12-09T00:20:36.811156Z","end":"2023-12-09T00:20:37.83793Z","steps":["trace[1465132326] 'process raft request'  (duration: 1.026519334s)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838169Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.931586Z","time spent":"906.571345ms","remote":"127.0.0.1:51808","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":1,"response size":4149,"request content":"key:\"/registry/deployments/kube-system/coredns\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.838256Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.810746Z","time spent":"1.027418538s","remote":"127.0.0.1:51540","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":853,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" mod_revision:1139 > success:<request_put:<key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" value_size:775 >> failure:<request_range:<key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" > >"}
{"level":"warn","ts":"2023-12-09T00:20:37.838322Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"898.767907ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" ","response":"range_response_count:1 size:7340"}
{"level":"warn","ts":"2023-12-09T00:20:37.838176Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"906.442245ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/exam/backend-deployment\" ","response":"range_response_count:1 size:3568"}
{"level":"warn","ts":"2023-12-09T00:20:37.838404Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"898.910908ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-controller-manager-minikube\" ","response":"range_response_count:1 size:6943"}
{"level":"info","ts":"2023-12-09T00:20:37.83847Z","caller":"traceutil/trace.go:171","msg":"trace[1997610436] range","detail":"{range_begin:/registry/pods/kube-system/kube-controller-manager-minikube; range_end:; response_count:1; response_revision:4422; }","duration":"898.978908ms","start":"2023-12-09T00:20:36.939482Z","end":"2023-12-09T00:20:37.838461Z","steps":["trace[1997610436] 'agreement among raft nodes before linearized reading'  (duration: 898.888908ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838504Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.939471Z","time spent":"899.023009ms","remote":"127.0.0.1:51558","response type":"/etcdserverpb.KV/Range","request count":0,"request size":61,"response count":1,"response size":6967,"request content":"key:\"/registry/pods/kube-system/kube-controller-manager-minikube\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.83855Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"904.957437ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:1432"}
{"level":"warn","ts":"2023-12-09T00:20:37.838585Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"904.932337ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/exam/backend-service\" ","response":"range_response_count:1 size:965"}
{"level":"info","ts":"2023-12-09T00:20:37.838594Z","caller":"traceutil/trace.go:171","msg":"trace[523538392] range","detail":"{range_begin:/registry/services/specs/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:4422; }","duration":"905.004137ms","start":"2023-12-09T00:20:36.933579Z","end":"2023-12-09T00:20:37.838584Z","steps":["trace[523538392] 'agreement among raft nodes before linearized reading'  (duration: 904.927837ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T00:20:37.83862Z","caller":"traceutil/trace.go:171","msg":"trace[877358999] range","detail":"{range_begin:/registry/services/specs/exam/backend-service; range_end:; response_count:1; response_revision:4422; }","duration":"904.966737ms","start":"2023-12-09T00:20:36.933643Z","end":"2023-12-09T00:20:37.83861Z","steps":["trace[877358999] 'agreement among raft nodes before linearized reading'  (duration: 904.906437ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838709Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.156939ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:1394"}
{"level":"info","ts":"2023-12-09T00:20:37.838746Z","caller":"traceutil/trace.go:171","msg":"trace[669494806] range","detail":"{range_begin:/registry/services/specs/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:4422; }","duration":"905.192939ms","start":"2023-12-09T00:20:36.933545Z","end":"2023-12-09T00:20:37.838738Z","steps":["trace[669494806] 'agreement among raft nodes before linearized reading'  (duration: 905.140138ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838745Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.133138ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/exam/frontend-service\" ","response":"range_response_count:1 size:960"}
{"level":"warn","ts":"2023-12-09T00:20:37.838784Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"899.613111ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/etcd-minikube\" ","response":"range_response_count:1 size:5282"}
{"level":"warn","ts":"2023-12-09T00:20:37.838803Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933518Z","time spent":"905.274839ms","remote":"127.0.0.1:51568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":1,"response size":1418,"request content":"key:\"/registry/services/specs/kubernetes-dashboard/kubernetes-dashboard\" "}
{"level":"info","ts":"2023-12-09T00:20:37.838817Z","caller":"traceutil/trace.go:171","msg":"trace[1545659278] range","detail":"{range_begin:/registry/services/specs/exam/frontend-service; range_end:; response_count:1; response_revision:4422; }","duration":"905.200138ms","start":"2023-12-09T00:20:36.9336Z","end":"2023-12-09T00:20:37.8388Z","steps":["trace[1545659278] 'agreement among raft nodes before linearized reading'  (duration: 905.101238ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838857Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"899.660411ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-scheduler-minikube\" ","response":"range_response_count:1 size:4195"}
{"level":"warn","ts":"2023-12-09T00:20:37.838854Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933595Z","time spent":"905.249138ms","remote":"127.0.0.1:51568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":984,"request content":"key:\"/registry/services/specs/exam/frontend-service\" "}
{"level":"info","ts":"2023-12-09T00:20:37.838881Z","caller":"traceutil/trace.go:171","msg":"trace[1955644935] range","detail":"{range_begin:/registry/pods/kube-system/kube-scheduler-minikube; range_end:; response_count:1; response_revision:4422; }","duration":"899.685611ms","start":"2023-12-09T00:20:36.939189Z","end":"2023-12-09T00:20:37.838874Z","steps":["trace[1955644935] 'agreement among raft nodes before linearized reading'  (duration: 899.636911ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838904Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.939174Z","time spent":"899.723512ms","remote":"127.0.0.1:51558","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":4219,"request content":"key:\"/registry/pods/kube-system/kube-scheduler-minikube\" "}
{"level":"info","ts":"2023-12-09T00:20:37.838355Z","caller":"traceutil/trace.go:171","msg":"trace[133627509] range","detail":"{range_begin:/registry/pods/kube-system/kube-apiserver-minikube; range_end:; response_count:1; response_revision:4422; }","duration":"898.800007ms","start":"2023-12-09T00:20:36.939546Z","end":"2023-12-09T00:20:37.838346Z","steps":["trace[133627509] 'agreement among raft nodes before linearized reading'  (duration: 898.734606ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838949Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.939538Z","time spent":"899.39911ms","remote":"127.0.0.1:51558","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":7364,"request content":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.838954Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933638Z","time spent":"905.084738ms","remote":"127.0.0.1:51568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":47,"response count":1,"response size":989,"request content":"key:\"/registry/services/specs/exam/backend-service\" "}
{"level":"info","ts":"2023-12-09T00:20:37.838436Z","caller":"traceutil/trace.go:171","msg":"trace[1259136559] range","detail":"{range_begin:/registry/deployments/exam/backend-deployment; range_end:; response_count:1; response_revision:4422; }","duration":"906.704846ms","start":"2023-12-09T00:20:36.93172Z","end":"2023-12-09T00:20:37.838425Z","steps":["trace[1259136559] 'agreement among raft nodes before linearized reading'  (duration: 906.399945ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.839067Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.816041ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/exam/mysql-statefulset\" ","response":"range_response_count:1 size:3155"}
{"level":"warn","ts":"2023-12-09T00:20:37.83907Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.93171Z","time spent":"907.348249ms","remote":"127.0.0.1:51808","response type":"/etcdserverpb.KV/Range","request count":0,"request size":47,"response count":1,"response size":3592,"request content":"key:\"/registry/deployments/exam/backend-deployment\" "}
{"level":"info","ts":"2023-12-09T00:20:37.839099Z","caller":"traceutil/trace.go:171","msg":"trace[850543751] range","detail":"{range_begin:/registry/statefulsets/exam/mysql-statefulset; range_end:; response_count:1; response_revision:4422; }","duration":"905.849741ms","start":"2023-12-09T00:20:36.93324Z","end":"2023-12-09T00:20:37.83909Z","steps":["trace[850543751] 'agreement among raft nodes before linearized reading'  (duration: 905.785541ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.839174Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933195Z","time spent":"905.967342ms","remote":"127.0.0.1:51816","response type":"/etcdserverpb.KV/Range","request count":0,"request size":47,"response count":1,"response size":3179,"request content":"key:\"/registry/statefulsets/exam/mysql-statefulset\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.838716Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.106638ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:4978"}
{"level":"info","ts":"2023-12-09T00:20:37.839226Z","caller":"traceutil/trace.go:171","msg":"trace[713189785] range","detail":"{range_begin:/registry/deployments/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:4422; }","duration":"905.60864ms","start":"2023-12-09T00:20:36.933603Z","end":"2023-12-09T00:20:37.839212Z","steps":["trace[713189785] 'agreement among raft nodes before linearized reading'  (duration: 905.087437ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.838673Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.028437ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:4958"}
{"level":"info","ts":"2023-12-09T00:20:37.839366Z","caller":"traceutil/trace.go:171","msg":"trace[154046913] range","detail":"{range_begin:/registry/deployments/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:4422; }","duration":"905.706941ms","start":"2023-12-09T00:20:36.933639Z","end":"2023-12-09T00:20:37.839345Z","steps":["trace[154046913] 'agreement among raft nodes before linearized reading'  (duration: 905.006137ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.839412Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933633Z","time spent":"905.764141ms","remote":"127.0.0.1:51808","response type":"/etcdserverpb.KV/Range","request count":0,"request size":65,"response count":1,"response size":4982,"request content":"key:\"/registry/deployments/kubernetes-dashboard/kubernetes-dashboard\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.838629Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933573Z","time spent":"905.045937ms","remote":"127.0.0.1:51568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":73,"response count":1,"response size":1456,"request content":"key:\"/registry/services/specs/kubernetes-dashboard/dashboard-metrics-scraper\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.839068Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"906.703345ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-node-lease/minikube\" ","response":"range_response_count:1 size:537"}
{"level":"info","ts":"2023-12-09T00:20:37.839746Z","caller":"traceutil/trace.go:171","msg":"trace[420037535] range","detail":"{range_begin:/registry/leases/kube-node-lease/minikube; range_end:; response_count:1; response_revision:4422; }","duration":"907.376349ms","start":"2023-12-09T00:20:36.932357Z","end":"2023-12-09T00:20:37.839733Z","steps":["trace[420037535] 'agreement among raft nodes before linearized reading'  (duration: 906.679845ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.83982Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.932346Z","time spent":"907.45865ms","remote":"127.0.0.1:51654","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":1,"response size":561,"request content":"key:\"/registry/leases/kube-node-lease/minikube\" "}
{"level":"info","ts":"2023-12-09T00:20:37.838812Z","caller":"traceutil/trace.go:171","msg":"trace[760019547] range","detail":"{range_begin:/registry/pods/kube-system/etcd-minikube; range_end:; response_count:1; response_revision:4422; }","duration":"899.664011ms","start":"2023-12-09T00:20:36.939139Z","end":"2023-12-09T00:20:37.838803Z","steps":["trace[760019547] 'agreement among raft nodes before linearized reading'  (duration: 899.594711ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.840042Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.939123Z","time spent":"900.903117ms","remote":"127.0.0.1:51558","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":5306,"request content":"key:\"/registry/pods/kube-system/etcd-minikube\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.83923Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"906.998947ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/exam/frontend-deployment\" ","response":"range_response_count:1 size:2681"}
{"level":"warn","ts":"2023-12-09T00:20:37.838667Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.097038ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/kube-system/kube-proxy\" ","response":"range_response_count:1 size:2895"}
{"level":"warn","ts":"2023-12-09T00:20:37.839008Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.53884ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" ","response":"range_response_count:1 size:5500"}
{"level":"warn","ts":"2023-12-09T00:20:37.839134Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"900.527116ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/exam/mysql-service\" ","response":"range_response_count:1 size:938"}
{"level":"warn","ts":"2023-12-09T00:20:37.839283Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"905.982842ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kube-system/kube-dns\" ","response":"range_response_count:1 size:1211"}
{"level":"warn","ts":"2023-12-09T00:20:37.839323Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933594Z","time spent":"905.715241ms","remote":"127.0.0.1:51808","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":5002,"request content":"key:\"/registry/deployments/kubernetes-dashboard/dashboard-metrics-scraper\" "}
{"level":"info","ts":"2023-12-09T00:20:37.845662Z","caller":"traceutil/trace.go:171","msg":"trace[1036773718] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:4422; }","duration":"912.186573ms","start":"2023-12-09T00:20:36.933458Z","end":"2023-12-09T00:20:37.845645Z","steps":["trace[1036773718] 'agreement among raft nodes before linearized reading'  (duration: 905.51184ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T00:20:37.845731Z","caller":"traceutil/trace.go:171","msg":"trace[281042435] range","detail":"{range_begin:/registry/services/specs/exam/mysql-service; range_end:; response_count:1; response_revision:4422; }","duration":"907.122548ms","start":"2023-12-09T00:20:36.938594Z","end":"2023-12-09T00:20:37.845717Z","steps":["trace[281042435] 'agreement among raft nodes before linearized reading'  (duration: 900.500815ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T00:20:37.845802Z","caller":"traceutil/trace.go:171","msg":"trace[280209347] range","detail":"{range_begin:/registry/deployments/exam/frontend-deployment; range_end:; response_count:1; response_revision:4422; }","duration":"913.573779ms","start":"2023-12-09T00:20:36.932218Z","end":"2023-12-09T00:20:37.845792Z","steps":["trace[280209347] 'agreement among raft nodes before linearized reading'  (duration: 906.971847ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.845839Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.938571Z","time spent":"907.254648ms","remote":"127.0.0.1:51568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":962,"request content":"key:\"/registry/services/specs/exam/mysql-service\" "}
{"level":"info","ts":"2023-12-09T00:20:37.845869Z","caller":"traceutil/trace.go:171","msg":"trace[1599768284] range","detail":"{range_begin:/registry/daemonsets/kube-system/kube-proxy; range_end:; response_count:1; response_revision:4422; }","duration":"912.300374ms","start":"2023-12-09T00:20:36.933561Z","end":"2023-12-09T00:20:37.845861Z","steps":["trace[1599768284] 'agreement among raft nodes before linearized reading'  (duration: 905.071138ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.845901Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933554Z","time spent":"912.337874ms","remote":"127.0.0.1:51828","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":2919,"request content":"key:\"/registry/daemonsets/kube-system/kube-proxy\" "}
{"level":"info","ts":"2023-12-09T00:20:37.845966Z","caller":"traceutil/trace.go:171","msg":"trace[386512717] range","detail":"{range_begin:/registry/services/specs/kube-system/kube-dns; range_end:; response_count:1; response_revision:4422; }","duration":"912.666075ms","start":"2023-12-09T00:20:36.93329Z","end":"2023-12-09T00:20:37.845956Z","steps":["trace[386512717] 'agreement among raft nodes before linearized reading'  (duration: 905.960142ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:37.846008Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.93324Z","time spent":"912.757275ms","remote":"127.0.0.1:51568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":47,"response count":1,"response size":1235,"request content":"key:\"/registry/services/specs/kube-system/kube-dns\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.845743Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.933435Z","time spent":"912.284273ms","remote":"127.0.0.1:51552","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":1,"response size":5524,"request content":"key:\"/registry/minions/minikube\" "}
{"level":"warn","ts":"2023-12-09T00:20:37.84584Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:36.932139Z","time spent":"913.690581ms","remote":"127.0.0.1:51808","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":2705,"request content":"key:\"/registry/deployments/exam/frontend-deployment\" "}
{"level":"warn","ts":"2023-12-09T00:20:39.827714Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"900.475115ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-12-09T00:20:39.827803Z","caller":"traceutil/trace.go:171","msg":"trace[1205285169] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4423; }","duration":"900.607215ms","start":"2023-12-09T00:20:38.927175Z","end":"2023-12-09T00:20:39.827782Z","steps":["trace[1205285169] 'range keys from in-memory index tree'  (duration: 900.280515ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T00:20:39.827859Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T00:20:38.927147Z","time spent":"900.694117ms","remote":"127.0.0.1:51382","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}

* 
* ==> etcd [32b392ce16e7] <==
* {"level":"info","ts":"2023-12-09T08:12:35.161143Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-12-09T08:12:35.161348Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-12-09T08:12:36.879537Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2023-12-09T08:12:36.879785Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2023-12-09T08:12:36.879857Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-12-09T08:12:36.879893Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2023-12-09T08:12:36.879916Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-12-09T08:12:36.879938Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2023-12-09T08:12:36.879956Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-12-09T08:12:36.986191Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-12-09T08:12:36.986288Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-09T08:12:36.986489Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-09T08:12:36.989071Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-12-09T08:12:36.989198Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-12-09T08:12:36.989591Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-12-09T08:12:36.989628Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-12-09T08:12:45.98999Z","caller":"traceutil/trace.go:171","msg":"trace[2007828773] transaction","detail":"{read_only:false; response_revision:4529; number_of_response:1; }","duration":"105.776535ms","start":"2023-12-09T08:12:45.884177Z","end":"2023-12-09T08:12:45.989953Z","steps":["trace[2007828773] 'process raft request'  (duration: 34.383642ms)","trace[2007828773] 'compare'  (duration: 71.235281ms)"],"step_count":2}
{"level":"info","ts":"2023-12-09T08:12:45.990816Z","caller":"traceutil/trace.go:171","msg":"trace[758339570] transaction","detail":"{read_only:false; response_revision:4530; number_of_response:1; }","duration":"100.543218ms","start":"2023-12-09T08:12:45.890252Z","end":"2023-12-09T08:12:45.990795Z","steps":["trace[758339570] 'process raft request'  (duration: 100.372305ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:12:46.300707Z","caller":"traceutil/trace.go:171","msg":"trace[170221477] transaction","detail":"{read_only:false; response_revision:4534; number_of_response:1; }","duration":"103.082621ms","start":"2023-12-09T08:12:46.197589Z","end":"2023-12-09T08:12:46.300672Z","steps":["trace[170221477] 'process raft request'  (duration: 102.800398ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:12:46.500151Z","caller":"traceutil/trace.go:171","msg":"trace[243144294] transaction","detail":"{read_only:false; response_revision:4535; number_of_response:1; }","duration":"293.94284ms","start":"2023-12-09T08:12:46.206159Z","end":"2023-12-09T08:12:46.500102Z","steps":["trace[243144294] 'process raft request'  (duration: 293.745625ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:12:46.712265Z","caller":"traceutil/trace.go:171","msg":"trace[1056575737] linearizableReadLoop","detail":"{readStateIndex:5417; appliedIndex:5416; }","duration":"206.354556ms","start":"2023-12-09T08:12:46.505873Z","end":"2023-12-09T08:12:46.712228Z","steps":["trace[1056575737] 'read index received'  (duration: 194.361499ms)","trace[1056575737] 'applied index is now lower than readState.Index'  (duration: 11.991256ms)"],"step_count":2}
{"level":"info","ts":"2023-12-09T08:12:46.712299Z","caller":"traceutil/trace.go:171","msg":"trace[166705921] transaction","detail":"{read_only:false; response_revision:4536; number_of_response:1; }","duration":"404.231835ms","start":"2023-12-09T08:12:46.308045Z","end":"2023-12-09T08:12:46.712276Z","steps":["trace[166705921] 'process raft request'  (duration: 392.107769ms)","trace[166705921] 'compare'  (duration: 11.858045ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-09T08:12:46.712445Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"206.625078ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kubernetes-dashboard/kubernetes-dashboard-8694d4445c-zlmnc\" ","response":"range_response_count:1 size:3989"}
{"level":"info","ts":"2023-12-09T08:12:46.7125Z","caller":"traceutil/trace.go:171","msg":"trace[914675560] range","detail":"{range_begin:/registry/pods/kubernetes-dashboard/kubernetes-dashboard-8694d4445c-zlmnc; range_end:; response_count:1; response_revision:4536; }","duration":"206.694083ms","start":"2023-12-09T08:12:46.505789Z","end":"2023-12-09T08:12:46.712483Z","steps":["trace[914675560] 'agreement among raft nodes before linearized reading'  (duration: 206.541371ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:12:46.712855Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T08:12:46.308012Z","time spent":"404.347445ms","remote":"127.0.0.1:56500","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":747,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/exam/frontend-deployment-777fcbc87-dtt2f.179f1ba1c156fd54\" mod_revision:0 > success:<request_put:<key:\"/registry/events/exam/frontend-deployment-777fcbc87-dtt2f.179f1ba1c156fd54\" value_size:655 lease:8128025692566292090 >> failure:<>"}
{"level":"info","ts":"2023-12-09T08:12:51.619337Z","caller":"traceutil/trace.go:171","msg":"trace[1822435591] transaction","detail":"{read_only:false; response_revision:4552; number_of_response:1; }","duration":"104.61283ms","start":"2023-12-09T08:12:51.514665Z","end":"2023-12-09T08:12:51.619278Z","steps":["trace[1822435591] 'process raft request'  (duration: 103.775404ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:12:52.110748Z","caller":"traceutil/trace.go:171","msg":"trace[1669032471] transaction","detail":"{read_only:false; response_revision:4553; number_of_response:1; }","duration":"240.971254ms","start":"2023-12-09T08:12:51.869729Z","end":"2023-12-09T08:12:52.110701Z","steps":["trace[1669032471] 'process raft request'  (duration: 240.741702ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:12:52.569138Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"237.707134ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/standard\" ","response":"range_response_count:1 size:992"}
{"level":"info","ts":"2023-12-09T08:12:52.569218Z","caller":"traceutil/trace.go:171","msg":"trace[227114237] range","detail":"{range_begin:/registry/storageclasses/standard; range_end:; response_count:1; response_revision:4554; }","duration":"237.836907ms","start":"2023-12-09T08:12:52.33136Z","end":"2023-12-09T08:12:52.569197Z","steps":["trace[227114237] 'range keys from in-memory index tree'  (duration: 237.543968ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:13:16.00771Z","caller":"traceutil/trace.go:171","msg":"trace[275168709] transaction","detail":"{read_only:false; response_revision:4673; number_of_response:1; }","duration":"116.618931ms","start":"2023-12-09T08:13:15.891054Z","end":"2023-12-09T08:13:16.007673Z","steps":["trace[275168709] 'process raft request'  (duration: 116.40023ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:13:57.488944Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.368439ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:126292"}
{"level":"info","ts":"2023-12-09T08:13:57.499701Z","caller":"traceutil/trace.go:171","msg":"trace[1029584982] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:4727; }","duration":"169.567788ms","start":"2023-12-09T08:13:57.330095Z","end":"2023-12-09T08:13:57.499663Z","steps":["trace[1029584982] 'range keys from bolt db'  (duration: 101.846152ms)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:14:04.966665Z","caller":"traceutil/trace.go:171","msg":"trace[259786838] transaction","detail":"{read_only:false; response_revision:4779; number_of_response:1; }","duration":"184.329667ms","start":"2023-12-09T08:14:04.781477Z","end":"2023-12-09T08:14:04.965807Z","steps":["trace[259786838] 'process raft request'  (duration: 184.098067ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:15:19.228796Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"150.726474ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13467"}
{"level":"info","ts":"2023-12-09T08:15:19.232238Z","caller":"traceutil/trace.go:171","msg":"trace[2077457102] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:4841; }","duration":"153.093951ms","start":"2023-12-09T08:15:19.078004Z","end":"2023-12-09T08:15:19.231098Z","steps":["trace[2077457102] 'range keys from in-memory index tree'  (duration: 150.539575ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:22:00.221634Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"692.377329ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-12-09T08:22:00.256901Z","caller":"traceutil/trace.go:171","msg":"trace[1642901709] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5367; }","duration":"722.067068ms","start":"2023-12-09T08:21:59.508281Z","end":"2023-12-09T08:22:00.230348Z","steps":["trace[1642901709] 'range keys from in-memory index tree'  (duration: 678.059459ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:22:00.259779Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T08:21:59.508203Z","time spent":"749.901931ms","remote":"127.0.0.1:41020","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-12-09T08:22:03.061177Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025692566296734,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-12-09T08:22:03.563197Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025692566296734,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-12-09T08:22:03.756873Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.195933095s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-12-09T08:22:03.756996Z","caller":"traceutil/trace.go:171","msg":"trace[1510910430] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5369; }","duration":"1.19608761s","start":"2023-12-09T08:22:02.560878Z","end":"2023-12-09T08:22:03.756966Z","steps":["trace[1510910430] 'agreement among raft nodes before linearized reading'  (duration: 1.195861988s)"],"step_count":1}
{"level":"warn","ts":"2023-12-09T08:22:03.757083Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T08:22:02.56086Z","time spent":"1.196197621s","remote":"127.0.0.1:41020","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-12-09T08:22:03.758561Z","caller":"traceutil/trace.go:171","msg":"trace[1776773156] transaction","detail":"{read_only:false; response_revision:5369; number_of_response:1; }","duration":"1.201280107s","start":"2023-12-09T08:22:02.549676Z","end":"2023-12-09T08:22:03.750957Z","steps":["trace[1776773156] 'process raft request'  (duration: 1.199411628s)"],"step_count":1}
{"level":"info","ts":"2023-12-09T08:22:03.749485Z","caller":"traceutil/trace.go:171","msg":"trace[311321979] linearizableReadLoop","detail":"{readStateIndex:6373; appliedIndex:6372; }","duration":"1.188503285s","start":"2023-12-09T08:22:02.560922Z","end":"2023-12-09T08:22:03.749425Z","steps":["trace[311321979] 'read index received'  (duration: 1.188125949s)","trace[311321979] 'applied index is now lower than readState.Index'  (duration: 375.335µs)"],"step_count":2}
{"level":"warn","ts":"2023-12-09T08:22:03.839352Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"434.913301ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/exam/mysql-data-mysql-statefulset-1\" ","response":"range_response_count:1 size:1245"}
{"level":"info","ts":"2023-12-09T08:22:03.839548Z","caller":"traceutil/trace.go:171","msg":"trace[139973582] range","detail":"{range_begin:/registry/persistentvolumeclaims/exam/mysql-data-mysql-statefulset-1; range_end:; response_count:1; response_revision:5369; }","duration":"435.081818ms","start":"2023-12-09T08:22:03.404397Z","end":"2023-12-09T08:22:03.839479Z","steps":["trace[139973582] 'agreement among raft nodes before linearized reading'  (duration: 370.366027ms)","trace[139973582] 'range keys from in-memory index tree'  (duration: 64.476167ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-09T08:22:03.839773Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T08:22:03.404375Z","time spent":"435.363145ms","remote":"127.0.0.1:56564","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1269,"request content":"key:\"/registry/persistentvolumeclaims/exam/mysql-data-mysql-statefulset-1\" "}
{"level":"warn","ts":"2023-12-09T08:22:03.840574Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.188732407s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-12-09T08:22:03.840637Z","caller":"traceutil/trace.go:171","msg":"trace[1556386452] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:5369; }","duration":"1.189214753s","start":"2023-12-09T08:22:02.651405Z","end":"2023-12-09T08:22:03.84062Z","steps":["trace[1556386452] 'agreement among raft nodes before linearized reading'  (duration: 1.123230841s)","trace[1556386452] 'count revisions from in-memory index tree'  (duration: 65.428859ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-09T08:22:03.840685Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T08:22:02.651386Z","time spent":"1.189279459s","remote":"127.0.0.1:56764","response type":"/etcdserverpb.KV/Range","request count":0,"request size":38,"response count":15,"response size":31,"request content":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true "}
{"level":"warn","ts":"2023-12-09T08:22:03.841171Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.055536ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:888"}
{"level":"info","ts":"2023-12-09T08:22:03.841225Z","caller":"traceutil/trace.go:171","msg":"trace[557140006] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:5369; }","duration":"177.109742ms","start":"2023-12-09T08:22:03.664094Z","end":"2023-12-09T08:22:03.841204Z","steps":["trace[557140006] 'agreement among raft nodes before linearized reading'  (duration: 110.647084ms)","trace[557140006] 'range keys from in-memory index tree'  (duration: 66.377549ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-09T08:22:03.841407Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.961892ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-12-09T08:22:03.841447Z","caller":"traceutil/trace.go:171","msg":"trace[1816603596] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:0; response_revision:5369; }","duration":"185.004896ms","start":"2023-12-09T08:22:03.656426Z","end":"2023-12-09T08:22:03.84143Z","steps":["trace[1816603596] 'agreement among raft nodes before linearized reading'  (duration: 118.326718ms)","trace[1816603596] 'count revisions from in-memory index tree'  (duration: 66.624273ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-09T08:22:04.020276Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-09T08:22:02.549626Z","time spent":"1.210117453s","remote":"127.0.0.1:56568","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:5368 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-12-09T08:22:38.918043Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5117}
{"level":"info","ts":"2023-12-09T08:22:39.310088Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5117,"took":"391.208233ms","hash":3229451292}
{"level":"info","ts":"2023-12-09T08:22:39.310515Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3229451292,"revision":5117,"compact-revision":3817}
{"level":"warn","ts":"2023-12-09T08:22:39.319219Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"153.874371ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025692566296888 > lease_revoke:<id:70cc8c4da2a998f9>","response":"size:29"}

* 
* ==> kernel <==
*  08:23:59 up 13 min,  0 users,  load average: 1.60, 1.53, 1.12
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [adaed48db95b] <==
* Trace[1101542653]: ---"About to write a response" 907ms (00:20:37.839)
Trace[1101542653]: [908.090953ms] [908.090953ms] END
I1209 00:20:37.840353       1 trace.go:236] Trace[282332187]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:73fdded0-df34-4db9-976a-60a820c6ff22,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/exam/services/backend-service,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 910ms):
Trace[282332187]: ---"About to write a response" 910ms (00:20:37.840)
Trace[282332187]: [910.386963ms] [910.386963ms] END
I1209 00:20:37.840433       1 trace.go:236] Trace[281009854]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:50b6b4bb-dd1c-4a62-accf-b2e3ad9cba21,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/exam/deployments/backend-deployment,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.930) (total time: 909ms):
Trace[281009854]: ---"About to write a response" 909ms (00:20:37.840)
Trace[281009854]: [909.57866ms] [909.57866ms] END
I1209 00:20:37.840634       1 trace.go:236] Trace[18225593]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:6d3f8171-4a90-42d0-8df4-3563f9c89832,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 908ms):
Trace[18225593]: ---"About to write a response" 908ms (00:20:37.840)
Trace[18225593]: [908.671256ms] [908.671256ms] END
I1209 00:20:37.840683       1 trace.go:236] Trace[1305894775]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:25064f67-fd0b-444a-b0d6-08dea07fe891,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/exam/services/frontend-service,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.930) (total time: 910ms):
Trace[1305894775]: ---"About to write a response" 910ms (00:20:37.840)
Trace[1305894775]: [910.531564ms] [910.531564ms] END
I1209 00:20:37.840368       1 trace.go:236] Trace[112318453]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:2b0451a7-1e9b-4e21-ad3e-1be6e3ca7ecf,client:192.168.49.2,protocol:HTTP/2.0,resource:statefulsets,scope:resource,url:/apis/apps/v1/namespaces/exam/statefulsets/mysql-statefulset,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 908ms):
Trace[112318453]: ---"About to write a response" 908ms (00:20:37.840)
Trace[112318453]: [908.954457ms] [908.954457ms] END
I1209 00:20:37.840882       1 trace.go:236] Trace[876363065]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f4f5a7fd-db6d-43ea-8597-da4cf249dba6,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/endpoints/kubernetes-dashboard,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (09-Dec-2023 00:20:36.806) (total time: 1034ms):
Trace[876363065]: ["GuaranteedUpdate etcd3" audit-id:f4f5a7fd-db6d-43ea-8597-da4cf249dba6,key:/services/endpoints/kubernetes-dashboard/kubernetes-dashboard,type:*core.Endpoints,resource:endpoints 1034ms (00:20:36.806)
Trace[876363065]:  ---"Txn call completed" 1032ms (00:20:37.840)]
Trace[876363065]: [1.034361472s] [1.034361472s] END
I1209 00:20:37.841017       1 trace.go:236] Trace[1470927299]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:38932414-de33-43d2-9f90-ab63ce2d4e14,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 909ms):
Trace[1470927299]: ---"About to write a response" 909ms (00:20:37.840)
Trace[1470927299]: [909.53356ms] [909.53356ms] END
I1209 00:20:37.841228       1 trace.go:236] Trace[833208221]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:f965ea33-8261-464d-968e-092e829bd563,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/coredns,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 911ms):
Trace[833208221]: ---"About to write a response" 911ms (00:20:37.841)
Trace[833208221]: [911.62267ms] [911.62267ms] END
I1209 00:20:37.841646       1 trace.go:236] Trace[1646950709]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:6e87af52-d335-4251-90dc-64c7c3d6897a,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 912ms):
Trace[1646950709]: ---"About to write a response" 912ms (00:20:37.841)
Trace[1646950709]: [912.528375ms] [912.528375ms] END
I1209 00:20:37.841670       1 trace.go:236] Trace[985460643]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:a2eaaf13-370d-43ef-b40b-b7e68403d959,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 912ms):
Trace[985460643]: ---"About to write a response" 912ms (00:20:37.841)
Trace[985460643]: [912.186173ms] [912.186173ms] END
I1209 00:20:37.841702       1 trace.go:236] Trace[175164953]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:7aab2eaa-adc3-461f-913b-14a184b002da,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kubernetes-dashboard/deployments/kubernetes-dashboard,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 909ms):
Trace[175164953]: ---"About to write a response" 909ms (00:20:37.841)
Trace[175164953]: [909.883061ms] [909.883061ms] END
I1209 00:20:37.842332       1 trace.go:236] Trace[11949531]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:76c6e652-29f5-48f0-94d8-3074aeaab1c5,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.932) (total time: 908ms):
Trace[11949531]: ---"About to write a response" 908ms (00:20:37.840)
Trace[11949531]: [908.461454ms] [908.461454ms] END
I1209 00:20:37.848425       1 trace.go:236] Trace[464605871]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:157ee72c-db85-413a-b321-b357ced74fa9,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/exam/services/mysql-service,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.930) (total time: 918ms):
Trace[464605871]: ---"About to write a response" 918ms (00:20:37.848)
Trace[464605871]: [918.203502ms] [918.203502ms] END
I1209 00:20:37.848614       1 trace.go:236] Trace[1646270435]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:5c699d17-d849-43ec-8c4a-a894c07b9671,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kubernetes-dashboard/deployments/dashboard-metrics-scraper,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 917ms):
Trace[1646270435]: ---"About to write a response" 917ms (00:20:37.848)
Trace[1646270435]: [917.462799ms] [917.462799ms] END
I1209 00:20:37.848677       1 trace.go:236] Trace[1329464662]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:fdbe7dd2-47bd-4a39-b2a6-76be0931b3e8,client:192.168.49.2,protocol:HTTP/2.0,resource:daemonsets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 916ms):
Trace[1329464662]: ---"About to write a response" 916ms (00:20:37.848)
Trace[1329464662]: [916.829396ms] [916.829396ms] END
I1209 00:20:37.848872       1 trace.go:236] Trace[271914093]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:d8a6aa8e-3f21-43bd-99d7-29d773c6b17f,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/kube-dns,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 919ms):
Trace[271914093]: ---"About to write a response" 919ms (00:20:37.848)
Trace[271914093]: [919.464408ms] [919.464408ms] END
I1209 00:20:37.848425       1 trace.go:236] Trace[19610132]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:9979a18e-ad82-4aa4-bfc3-d22b64b87fd2,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/exam/deployments/frontend-deployment,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 918ms):
Trace[19610132]: ---"About to write a response" 918ms (00:20:37.848)
Trace[19610132]: [918.544904ms] [918.544904ms] END
I1209 00:20:37.850465       1 trace.go:236] Trace[882608371]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:ccf8dbab-45bc-4e4c-a69b-e538d2a27594,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/etcd-minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.929) (total time: 920ms):
Trace[882608371]: ---"About to write a response" 920ms (00:20:37.850)
Trace[882608371]: [920.718015ms] [920.718015ms] END
I1209 00:20:37.850644       1 trace.go:236] Trace[2135722701]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:8b84b554-3554-4e9e-9954-9f2bc239ccdd,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (09-Dec-2023 00:20:36.931) (total time: 919ms):
Trace[2135722701]: ---"About to write a response" 918ms (00:20:37.850)
Trace[2135722701]: [919.049307ms] [919.049307ms] END

* 
* ==> kube-apiserver [b759b7c3a44b] <==
* E1209 08:15:32.535100       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:15:42.535666       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:15:52.538413       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E1209 08:16:02.539794       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:16:12.541089       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:16:22.542269       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:16:32.543666       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E1209 08:16:42.544434       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E1209 08:16:52.546036       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:17:02.547329       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:17:12.548256       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:17:22.549413       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:17:32.551498       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:17:42.552218       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E1209 08:17:52.552712       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:18:02.554005       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:18:12.554880       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:18:22.555061       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:18:32.556572       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E1209 08:18:42.566371       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:18:52.567806       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:02.569299       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:12.570041       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:22.570263       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:32.570644       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:42.571653       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:52.572519       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E1209 08:19:58.585042       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1209 08:19:58.587420       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1209 08:19:58.587528       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1209 08:19:58.591095       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1209 08:19:58.592528       1 timeout.go:142] post-timeout activity - time-elapsed: 12.156942ms, GET "/api/v1/namespaces/ingress-nginx/pods" result: <nil>
E1209 08:20:02.572864       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E1209 08:20:12.574119       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:20:22.574313       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:20:32.575280       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E1209 08:20:42.576196       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","global-default","leader-election","node-high","system","workload-high","workload-low","catch-all"] items=[{},{},{},{},{},{},{},{}]
E1209 08:20:52.568618       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:21:02.569913       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:21:12.571216       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E1209 08:21:22.569180       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:21:32.571374       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E1209 08:21:42.572119       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:21:52.572224       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:22:02.579809       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","leader-election","node-high","system","workload-high","workload-low","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
I1209 08:22:04.071283       1 trace.go:236] Trace[1051982135]: "Update" accept:application/json, */*,audit-id:2420c673-94a9-415d-a6d7-cce09531f89e,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Dec-2023 08:22:02.450) (total time: 1570ms):
Trace[1051982135]: ["GuaranteedUpdate etcd3" audit-id:2420c673-94a9-415d-a6d7-cce09531f89e,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1569ms (08:22:02.451)
Trace[1051982135]:  ---"Txn call completed" 1567ms (08:22:04.021)]
Trace[1051982135]: [1.570704071s] [1.570704071s] END
E1209 08:22:12.581972       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E1209 08:22:22.582593       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E1209 08:22:32.583255       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:22:42.584177       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E1209 08:22:52.584707       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:23:02.585633       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","node-high","system","workload-high","workload-low","catch-all","exempt","global-default"] items=[{},{},{},{},{},{},{},{}]
E1209 08:23:12.586337       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:23:22.586993       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","catch-all","exempt","global-default","leader-election","node-high","system","workload-high"] items=[{},{},{},{},{},{},{},{}]
E1209 08:23:32.587792       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","system","workload-high","workload-low","catch-all","exempt","global-default","leader-election"] items=[{},{},{},{},{},{},{},{}]
E1209 08:23:42.588656       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","catch-all","exempt","global-default","leader-election","node-high","system"] items=[{},{},{},{},{},{},{},{}]
E1209 08:23:52.589718       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","global-default","leader-election","node-high","system","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]

* 
* ==> kube-controller-manager [9986e447445a] <==
* I1209 00:20:36.483623       1 shared_informer.go:318] Caches are synced for cronjob
I1209 00:20:36.483678       1 shared_informer.go:318] Caches are synced for TTL after finished
I1209 00:20:36.488407       1 shared_informer.go:318] Caches are synced for namespace
I1209 00:20:36.488479       1 shared_informer.go:318] Caches are synced for service account
I1209 00:20:36.488773       1 shared_informer.go:318] Caches are synced for TTL
I1209 00:20:36.488859       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1209 00:20:36.489112       1 shared_informer.go:318] Caches are synced for node
I1209 00:20:36.489195       1 range_allocator.go:174] "Sending events to api server"
I1209 00:20:36.489236       1 range_allocator.go:178] "Starting range CIDR allocator"
I1209 00:20:36.489246       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1209 00:20:36.489257       1 shared_informer.go:318] Caches are synced for cidrallocator
I1209 00:20:36.505572       1 shared_informer.go:318] Caches are synced for expand
I1209 00:20:36.520941       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1209 00:20:36.521197       1 shared_informer.go:318] Caches are synced for crt configmap
I1209 00:20:36.526335       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1209 00:20:36.601208       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1209 00:20:36.601301       1 shared_informer.go:318] Caches are synced for GC
I1209 00:20:36.601444       1 shared_informer.go:318] Caches are synced for PV protection
I1209 00:20:36.601510       1 shared_informer.go:318] Caches are synced for resource quota
I1209 00:20:36.601574       1 shared_informer.go:318] Caches are synced for HPA
I1209 00:20:36.601629       1 shared_informer.go:318] Caches are synced for attach detach
I1209 00:20:36.603434       1 shared_informer.go:318] Caches are synced for PVC protection
I1209 00:20:36.604052       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1209 00:20:36.604121       1 shared_informer.go:318] Caches are synced for stateful set
I1209 00:20:36.605218       1 shared_informer.go:318] Caches are synced for resource quota
I1209 00:20:36.605293       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1209 00:20:36.605279       1 shared_informer.go:318] Caches are synced for job
I1209 00:20:36.605428       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1209 00:20:36.605646       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="168.7µs"
I1209 00:20:36.605732       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-77dfbcc67b" duration="49.4µs"
I1209 00:20:36.605845       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="56.701µs"
I1209 00:20:36.605958       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="93.7µs"
I1209 00:20:36.606018       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="42.5µs"
I1209 00:20:36.606093       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="56.301µs"
I1209 00:20:36.606119       1 shared_informer.go:318] Caches are synced for ephemeral
I1209 00:20:36.606171       1 shared_informer.go:318] Caches are synced for persistent volume
I1209 00:20:36.606412       1 shared_informer.go:318] Caches are synced for deployment
I1209 00:20:36.607865       1 shared_informer.go:318] Caches are synced for ReplicationController
I1209 00:20:36.607949       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1209 00:20:36.611603       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1209 00:20:36.611770       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1209 00:20:36.612840       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1209 00:20:36.617395       1 shared_informer.go:318] Caches are synced for disruption
I1209 00:20:36.617459       1 shared_informer.go:318] Caches are synced for endpoint
I1209 00:20:36.617524       1 shared_informer.go:318] Caches are synced for daemon sets
I1209 00:20:36.619313       1 shared_informer.go:318] Caches are synced for taint
I1209 00:20:36.619398       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I1209 00:20:36.619503       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1209 00:20:36.619554       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I1209 00:20:36.619575       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1209 00:20:36.619597       1 taint_manager.go:211] "Sending events to api server"
I1209 00:20:36.620456       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1209 00:20:36.848950       1 shared_informer.go:318] Caches are synced for garbage collector
I1209 00:20:36.849046       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1209 00:20:36.927548       1 shared_informer.go:318] Caches are synced for garbage collector
I1209 00:20:42.555840       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="146.458619ms"
I1209 00:20:42.556038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="92.801µs"
I1209 00:20:47.256708       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="216.801µs"
I1209 00:21:03.259809       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="54.36887ms"
I1209 00:21:03.262038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="155.9µs"

* 
* ==> kube-controller-manager [d13654ff003b] <==
* I1209 08:15:51.866447       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="111.801µs"
I1209 08:15:51.879443       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="81.901µs"
I1209 08:15:52.506177       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="69.201µs"
I1209 08:15:52.816548       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="67.6µs"
I1209 08:15:52.841821       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="80.101µs"
I1209 08:15:52.854586       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/frontend-deployment-777fcbc87" duration="92.901µs"
I1209 08:15:56.258852       1 event.go:307] "Event occurred" object="exam/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-deployment-767bfdd69 to 1"
I1209 08:15:56.275079       1 event.go:307] "Event occurred" object="exam/backend-deployment-767bfdd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-767bfdd69-wdz72"
I1209 08:15:56.303654       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="44.849743ms"
I1209 08:15:56.318377       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="14.577042ms"
I1209 08:15:56.319885       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="1.407198ms"
I1209 08:15:56.336250       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="130.19µs"
I1209 08:15:58.145477       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="15.515174ms"
I1209 08:15:58.145787       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="119.691µs"
I1209 08:15:58.167232       1 event.go:307] "Event occurred" object="exam/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set backend-deployment-6bb8f846b7 to 1 from 2"
I1209 08:15:58.187440       1 event.go:307] "Event occurred" object="exam/backend-deployment-6bb8f846b7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: backend-deployment-6bb8f846b7-km54x"
I1209 08:15:58.201365       1 event.go:307] "Event occurred" object="exam/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-deployment-767bfdd69 to 2 from 1"
I1209 08:15:58.211180       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="44.0713ms"
I1209 08:15:58.219032       1 event.go:307] "Event occurred" object="exam/backend-deployment-767bfdd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-767bfdd69-tr5sg"
I1209 08:15:58.243085       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="31.782492ms"
I1209 08:15:58.267993       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="193.586µs"
I1209 08:15:58.270654       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="69.849728ms"
I1209 08:15:58.281563       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="10.763219ms"
I1209 08:15:58.282006       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="158.988µs"
I1209 08:15:58.294287       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="83.694µs"
I1209 08:15:59.339972       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="96.4µs"
I1209 08:16:00.599167       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="73.701µs"
I1209 08:16:00.632052       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="68.8µs"
I1209 08:16:00.677278       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="89.7µs"
I1209 08:16:00.794478       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="78.330296ms"
I1209 08:16:00.794805       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-767bfdd69" duration="132.501µs"
I1209 08:16:00.811536       1 event.go:307] "Event occurred" object="exam/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set backend-deployment-6bb8f846b7 to 0 from 1"
I1209 08:16:00.869833       1 event.go:307] "Event occurred" object="exam/backend-deployment-6bb8f846b7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: backend-deployment-6bb8f846b7-wwkg6"
I1209 08:16:00.927022       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="117.068893ms"
I1209 08:16:00.975356       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="48.167521ms"
I1209 08:16:00.975543       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="79.8µs"
I1209 08:16:01.969284       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="124.6µs"
I1209 08:16:03.010787       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="69.4µs"
I1209 08:16:03.070348       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="82.8µs"
I1209 08:16:03.090139       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="exam/backend-deployment-6bb8f846b7" duration="113.3µs"
I1209 08:16:43.809264       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:16:54.167864       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:16:54.594983       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:17:07.468555       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:17:23.439649       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:17:31.494773       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:17:34.428144       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:17:45.422264       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:17:56.432517       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:18:07.428984       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:18:07.450455       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:18:19.432378       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:19:04.438993       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:19:11.427659       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:19:16.447606       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:19:22.436362       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:21:43.406471       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:21:55.485427       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I1209 08:21:56.474360       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I1209 08:22:07.412160       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"

* 
* ==> kube-proxy [b1dd8e20c361] <==
* I1209 00:20:28.745892       1 server_others.go:69] "Using iptables proxy"
I1209 00:20:28.875584       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1209 00:20:29.107434       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1209 00:20:29.112580       1 server_others.go:152] "Using iptables Proxier"
I1209 00:20:29.112646       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1209 00:20:29.112665       1 server_others.go:438] "Defaulting to no-op detect-local"
I1209 00:20:29.115347       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1209 00:20:29.115888       1 server.go:846] "Version info" version="v1.28.3"
I1209 00:20:29.115925       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1209 00:20:29.131796       1 config.go:188] "Starting service config controller"
I1209 00:20:29.136270       1 config.go:97] "Starting endpoint slice config controller"
I1209 00:20:29.137130       1 config.go:315] "Starting node config controller"
I1209 00:20:29.145952       1 shared_informer.go:311] Waiting for caches to sync for node config
I1209 00:20:29.140041       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1209 00:20:29.142038       1 shared_informer.go:311] Waiting for caches to sync for service config
I1209 00:20:29.246523       1 shared_informer.go:318] Caches are synced for service config
I1209 00:20:29.246602       1 shared_informer.go:318] Caches are synced for node config
I1209 00:20:29.246621       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [e94d4c14d9b9] <==
* I1209 08:12:56.318557       1 server_others.go:69] "Using iptables proxy"
I1209 08:12:56.544485       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1209 08:12:57.276270       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1209 08:12:57.309015       1 server_others.go:152] "Using iptables Proxier"
I1209 08:12:57.309188       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1209 08:12:57.309213       1 server_others.go:438] "Defaulting to no-op detect-local"
I1209 08:12:57.309648       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1209 08:12:57.312010       1 server.go:846] "Version info" version="v1.28.3"
I1209 08:12:57.312049       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1209 08:12:57.368538       1 config.go:97] "Starting endpoint slice config controller"
I1209 08:12:57.368891       1 config.go:188] "Starting service config controller"
I1209 08:12:57.374199       1 shared_informer.go:311] Waiting for caches to sync for service config
I1209 08:12:57.377999       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1209 08:12:57.383903       1 config.go:315] "Starting node config controller"
I1209 08:12:57.383973       1 shared_informer.go:311] Waiting for caches to sync for node config
I1209 08:12:57.584684       1 shared_informer.go:318] Caches are synced for node config
I1209 08:12:57.587264       1 shared_informer.go:318] Caches are synced for service config
I1209 08:12:57.587376       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [259f877b7f95] <==
* I1209 00:20:17.740572       1 serving.go:348] Generated self-signed cert in-memory
W1209 00:20:21.460524       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1209 00:20:21.460578       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1209 00:20:21.460601       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1209 00:20:21.460615       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1209 00:20:21.520483       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1209 00:20:21.520642       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1209 00:20:21.526700       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1209 00:20:21.526819       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1209 00:20:21.528780       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1209 00:20:21.529475       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1209 00:20:21.827184       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [90db0a937b91] <==
* I1209 08:12:37.570314       1 serving.go:348] Generated self-signed cert in-memory
W1209 08:12:42.314190       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1209 08:12:42.314373       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1209 08:12:42.314443       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1209 08:12:42.314468       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1209 08:12:42.508519       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1209 08:12:42.508697       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1209 08:12:42.513343       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1209 08:12:42.514121       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1209 08:12:42.515560       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1209 08:12:42.515673       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1209 08:12:42.614806       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Dec 09 08:18:19 minikube kubelet[1724]: E1209 08:18:19.422633    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:18:22 minikube kubelet[1724]: E1209 08:18:22.414737    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:18:34 minikube kubelet[1724]: E1209 08:18:34.413634    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:18:49 minikube kubelet[1724]: E1209 08:18:49.466703    1724 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:18:49 minikube kubelet[1724]: E1209 08:18:49.466840    1724 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:18:49 minikube kubelet[1724]: E1209 08:18:49.467359    1724 kuberuntime_manager.go:1256] container &Container{Name:patch,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80,Command:[],Args:[patch --webhook-name=ingress-nginx-admission --namespace=$(POD_NAMESPACE) --patch-mutating=false --secret-name=ingress-nginx-admission --patch-failure-policy=Fail],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqdtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-patch-m657d_ingress-nginx(873d708a-ec40-48bd-9b84-1bdb6f896879): ErrImagePull: Error response from daemon: Get "https://registry.k8s.io/v2/": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
Dec 09 08:18:49 minikube kubelet[1724]: E1209 08:18:49.467463    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:18:59 minikube kubelet[1724]: E1209 08:18:59.520477    1724 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:18:59 minikube kubelet[1724]: E1209 08:18:59.520573    1724 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:18:59 minikube kubelet[1724]: E1209 08:18:59.520894    1724 kuberuntime_manager.go:1256] container &Container{Name:create,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80,Command:[],Args:[create --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc --namespace=$(POD_NAMESPACE) --secret-name=ingress-nginx-admission],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zctk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-create-xd5vp_ingress-nginx(b52bb9bc-ea98-40cd-ade7-39bbfb179cd5): ErrImagePull: Error response from daemon: Get "https://registry.k8s.io/v2/": net/http: TLS handshake timeout
Dec 09 08:18:59 minikube kubelet[1724]: E1209 08:18:59.520982    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": net/http: TLS handshake timeout\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:19:04 minikube kubelet[1724]: E1209 08:19:04.407327    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:19:11 minikube kubelet[1724]: E1209 08:19:11.407150    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:19:16 minikube kubelet[1724]: E1209 08:19:16.419443    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:19:22 minikube kubelet[1724]: E1209 08:19:22.412027    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:19:30 minikube kubelet[1724]: E1209 08:19:30.406523    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:19:33 minikube kubelet[1724]: E1209 08:19:33.405928    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:19:43 minikube kubelet[1724]: E1209 08:19:43.407216    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:19:47 minikube kubelet[1724]: E1209 08:19:47.406668    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:19:54 minikube kubelet[1724]: E1209 08:19:54.406384    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:19:59 minikube kubelet[1724]: E1209 08:19:59.406387    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:20:08 minikube kubelet[1724]: E1209 08:20:08.406154    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:20:11 minikube kubelet[1724]: E1209 08:20:11.056868    1724 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Dec 09 08:20:11 minikube kubelet[1724]: E1209 08:20:11.057043    1724 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/461b49e6-f3c5-4d65-b029-a4e590bb6944-webhook-cert podName:461b49e6-f3c5-4d65-b029-a4e590bb6944 nodeName:}" failed. No retries permitted until 2023-12-09 08:22:13.057016201 +0000 UTC m=+585.819857596 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/461b49e6-f3c5-4d65-b029-a4e590bb6944-webhook-cert") pod "ingress-nginx-controller-7c6974c4d8-dpkg2" (UID: "461b49e6-f3c5-4d65-b029-a4e590bb6944") : secret "ingress-nginx-admission" not found
Dec 09 08:20:11 minikube kubelet[1724]: E1209 08:20:11.405818    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:20:37 minikube kubelet[1724]: E1209 08:20:37.409855    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-dpkg2" podUID="461b49e6-f3c5-4d65-b029-a4e590bb6944"
Dec 09 08:21:33 minikube kubelet[1724]: E1209 08:21:33.076374    1724 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:21:33 minikube kubelet[1724]: E1209 08:21:33.076460    1724 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:21:33 minikube kubelet[1724]: E1209 08:21:33.076963    1724 kuberuntime_manager.go:1256] container &Container{Name:patch,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80,Command:[],Args:[patch --webhook-name=ingress-nginx-admission --namespace=$(POD_NAMESPACE) --patch-mutating=false --secret-name=ingress-nginx-admission --patch-failure-policy=Fail],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqdtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-patch-m657d_ingress-nginx(873d708a-ec40-48bd-9b84-1bdb6f896879): ErrImagePull: rpc error: code = Canceled desc = context canceled
Dec 09 08:21:33 minikube kubelet[1724]: E1209 08:21:33.077089    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:21:43 minikube kubelet[1724]: E1209 08:21:43.113598    1724 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:21:43 minikube kubelet[1724]: E1209 08:21:43.113917    1724 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: TLS handshake timeout" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80"
Dec 09 08:21:43 minikube kubelet[1724]: E1209 08:21:43.114292    1724 kuberuntime_manager.go:1256] container &Container{Name:create,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80,Command:[],Args:[create --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc --namespace=$(POD_NAMESPACE) --secret-name=ingress-nginx-admission],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zctk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-create-xd5vp_ingress-nginx(b52bb9bc-ea98-40cd-ade7-39bbfb179cd5): ErrImagePull: Error response from daemon: Get "https://registry.k8s.io/v2/": net/http: TLS handshake timeout
Dec 09 08:21:43 minikube kubelet[1724]: E1209 08:21:43.114377    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": net/http: TLS handshake timeout\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:21:43 minikube kubelet[1724]: E1209 08:21:43.389452    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:21:55 minikube kubelet[1724]: E1209 08:21:55.390202    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:21:56 minikube kubelet[1724]: E1209 08:21:56.389722    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:22:07 minikube kubelet[1724]: E1209 08:22:07.388899    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:22:07 minikube kubelet[1724]: E1209 08:22:07.388989    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:22:13 minikube kubelet[1724]: E1209 08:22:13.050421    1724 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Dec 09 08:22:13 minikube kubelet[1724]: E1209 08:22:13.050620    1724 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/461b49e6-f3c5-4d65-b029-a4e590bb6944-webhook-cert podName:461b49e6-f3c5-4d65-b029-a4e590bb6944 nodeName:}" failed. No retries permitted until 2023-12-09 08:24:15.05057876 +0000 UTC m=+707.831106674 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/461b49e6-f3c5-4d65-b029-a4e590bb6944-webhook-cert") pod "ingress-nginx-controller-7c6974c4d8-dpkg2" (UID: "461b49e6-f3c5-4d65-b029-a4e590bb6944") : secret "ingress-nginx-admission" not found
Dec 09 08:22:19 minikube kubelet[1724]: E1209 08:22:19.391760    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:22:21 minikube kubelet[1724]: E1209 08:22:21.388481    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:22:28 minikube kubelet[1724]: W1209 08:22:28.510699    1724 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 09 08:22:30 minikube kubelet[1724]: E1209 08:22:30.389943    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:22:35 minikube kubelet[1724]: E1209 08:22:35.387981    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:22:45 minikube kubelet[1724]: E1209 08:22:45.390669    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:22:47 minikube kubelet[1724]: E1209 08:22:47.390755    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:22:54 minikube kubelet[1724]: E1209 08:22:54.385027    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-dpkg2" podUID="461b49e6-f3c5-4d65-b029-a4e590bb6944"
Dec 09 08:22:59 minikube kubelet[1724]: E1209 08:22:59.387996    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:22:59 minikube kubelet[1724]: E1209 08:22:59.388138    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:23:11 minikube kubelet[1724]: E1209 08:23:11.389373    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:23:13 minikube kubelet[1724]: E1209 08:23:13.390221    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:23:22 minikube kubelet[1724]: E1209 08:23:22.387714    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:23:27 minikube kubelet[1724]: E1209 08:23:27.388532    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:23:34 minikube kubelet[1724]: E1209 08:23:34.388843    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:23:39 minikube kubelet[1724]: E1209 08:23:39.387823    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:23:45 minikube kubelet[1724]: E1209 08:23:45.387986    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"
Dec 09 08:23:54 minikube kubelet[1724]: E1209 08:23:54.389402    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-xd5vp" podUID="b52bb9bc-ea98-40cd-ade7-39bbfb179cd5"
Dec 09 08:23:58 minikube kubelet[1724]: E1209 08:23:58.389068    1724 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-m657d" podUID="873d708a-ec40-48bd-9b84-1bdb6f896879"

* 
* ==> kubernetes-dashboard [51cf56730f4e] <==
* 2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:12 [2023-12-09T08:21:12Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of namespaces
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/cronjob/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all cron jobs in the cluster
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/daemonset/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/deployment/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all deployments in the cluster
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/job/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all jobs in the cluster
2023/12/09 08:21:13 received 0 resources from sidecar instead of 4
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 received 0 resources from sidecar instead of 4
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/replicaset/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all replica sets in the cluster
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/pod/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all pods in the cluster
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all replication controllers in the cluster
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Incoming HTTP/1.1 GET /api/v1/statefulset/exam?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/09 08:21:13 Getting list of all pet sets in the cluster
2023/12/09 08:21:13 received 0 resources from sidecar instead of 6
2023/12/09 08:21:13 received 0 resources from sidecar instead of 4
2023/12/09 08:21:13 received 0 resources from sidecar instead of 2
2023/12/09 08:21:13 received 0 resources from sidecar instead of 6
2023/12/09 08:21:13 Getting pod metrics
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 received 0 resources from sidecar instead of 4
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 received 0 resources from sidecar instead of 2
2023/12/09 08:21:13 received 0 resources from sidecar instead of 6
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/09 08:21:13 received 0 resources from sidecar instead of 6
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 Skipping metric because of error: Metric label not set.
2023/12/09 08:21:13 [2023-12-09T08:21:13Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> kubernetes-dashboard [99fc26335607] <==
* 2023/12/09 08:12:57 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00071fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000506600)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2023/12/09 08:12:57 Using namespace: kubernetes-dashboard
2023/12/09 08:12:57 Using in-cluster config to connect to apiserver
2023/12/09 08:12:57 Using secret token for csrf signing
2023/12/09 08:12:57 Initializing csrf token from kubernetes-dashboard-csrf secret

* 
* ==> storage-provisioner [95f47218fb3b] <==
* I1209 08:13:19.623547       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1209 08:13:19.787509       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1209 08:13:19.790125       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1209 08:13:37.672620       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1209 08:13:37.673406       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e061e9aa-37f0-4ce7-af64-20eecf575383!
I1209 08:13:37.706405       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"f43fa959-7584-4945-80f8-c7ea43c93779", APIVersion:"v1", ResourceVersion:"4695", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e061e9aa-37f0-4ce7-af64-20eecf575383 became leader
I1209 08:13:37.927379       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e061e9aa-37f0-4ce7-af64-20eecf575383!

* 
* ==> storage-provisioner [e259ef5e466a] <==
* I1209 08:12:54.811791       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1209 08:13:05.080768       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

